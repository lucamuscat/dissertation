\chapter{Background \& Literature Review}
Common intuitions programmers may have about sequential programs rarely
carry-over to concurrent ones. This chapter guides the reader through the basic
concepts required to understand the contributions of this study. 
Firstly, a discussion on the differences between \emph{uniprocessors} and
\emph{multiprocessors} is given. Secondly, multiprocessor programming concepts
are discussed. Lastly, a definition of queues and concurrent queues is provided.

\section{Multiprocessors}
The diminishing growth in \emph{uniprocessor} speeds
forced programmers to look at parallelism as a means of improving
performance~\citep{cantrill2008real}. Unlike uniprocessors, \emph{multiprocessors} do
not ensure that instructions appear in execution order~\cite{scott2013shared},
making multiprocessors hard to model.

% \emph{Multiprocessors} consist of multiple hardware processors, each
% executing a \emph{sequential program}~\citep[Appendix~B.2]{herlihy2020art}.

Over the years, multiprocessor programming has become more practical and
feasible, partly thanks to multiprocessors getting cheaper, and
easier to program due to abstractions such as OpenMP, MPI (Message Passing
Interface), implicit parallel programming environments such as SQL, and
programming language standards that provide guarantees as to how parallel code will
behave, such as Java's\citep{javamemorymodel2014} and C++11's
\citep{cppmemorymodel} standardized memory models
\citep[Chapter~2.2]{perfbook2021}.

\emph{Multiprocessors} execute a number of \emph{processes} on distinct
hardware processors~\citep[Appendix~B.2]{herlihy2020art}, with each processor
continuously executing, and swapping out \emph{processes}.
\citeauthor{scott2013shared} defines a \emph{process} as a set of
\emph{threads}, where a thread is an active computation that has the potential
to share variables with other, concurrent threads~\citep[p.6]{scott2013shared}.
Once a process' \emph{time quantum} (fixed share of CPU time) expires, the
process is \emph{context-switched}. Processes
may be prematurely \emph{de-scheduled} due to interrupts,
system calls, or through the voluntary yielding of control to the CPU~\citep[Section~3.2.3]{osconcepts2021}.

% \citep[Appendix~B.7.1]{perfbook2021}.
For the sake of optimization, Compilers and CPUs do not guarantee specific
results in cases where multiple threads concurrently modify the same memory
location~\citep{drepper2007every}. The explicit use synchronization primitives
(such as read-modify-write operations, or memory barriers) are needed to
prevent incoherence across caches, which may eventually lead to the corruption
of data.

\section{Synchronization}
\subsection{CPU Cache}
As the disparity between CPU frequency and memory access times grew, 
CPU engineers added small, yet fast SRAM chips between the CPU
and main memory in order to alleviate issues caused by the differences in speed
~\citep{cantrill2008real,drepper2007every,perfbook2021}. Using large
quantities of SRAM is considered to be uneconomical, ergo, more cache layers
were introduced, with each layer being cheaper, larger, and slower than the
last~\citep{drepper2007every,perfbook2021}.

Data is transferred in units of \emph{cache lines}, which are power-of-two
fixed-size aligned blocks of memory, commonly ranging from 32 to 256 bytes in
size~\citep{perfbook2021_cacheline}. Cache lines are filled with data
starting from the requested memory address, together with neighbouring bytes~\citep{perfbook2021_cacheline}.

% The \emph{MESI} cache-coherence protocol (Modified, Exclusive, Shared, Invalid)
% attaches states to memory addresses; transitions notify the processor
% when to invalidate a cache line in order to prevent stale data~\citep[Appendix~B.5.1]{herlihy2020art}.

\emph{Cache-Incoherence} is a problem where distinct caches contain stale
data~\citep{herlihy2020art_cachecoherence}. A memory address
may be safely cached in different processors if no updates occur; 
cache lines will be invalidated if the contents of the shared memory address are
updated (preventing stale data from being
read)~\citep{herlihy2020art_cachecoherence}.
The \emph{MESI} cache-coherence protocol (Modified, Exclusive, Shared, or
Invalid) attaches states to memory
addresses~\citep{herlihy2020art_cachecoherence}. Transitions in state notify
the processor when to invalidate a cache
line~\citep{herlihy2020art_cachecoherence}.

%\emph{Sharing} occurs when overlapping data is loaded into several cache lines.
\emph{Sharing} occurs when one processor reads or writes to a memory address that
is cached by another~\citep{herlihy2020art_cachecoherence}. In some cases,
sharing is unavoidable; closely located, yet
logically distinct data may share a single cache
line~\citep{herlihy2020art_cachecoherence}. 

\emph{False Sharing} takes place when cache lines are invalidated due to updates
on logically distinct data~\citep{herlihy2020art_cachecoherence}. As the
rate of invalidations increases with each write to shared-memory, performance
tends to degrade heavily, harming scalability. At the cost of a higher memory footprint, false
sharing may be solved by segregating logically distinct data with unused memory
to prevent the sharing of cache lines~\citep{scott2013shared}.

\subsection{Interconnection}

% TODO: Not really happy with the UMA definition, will need to give a better explanation.
An interconnection medium is used to connect CPUs to cache and memory; where
two kinds of interconnection architectures exist: (1)
\emph{Nonuniform memory access (NUMA)}
architectures~\citep[Appendix~B.3]{herlihy2020art} are typically associated with
multi-CPU and distributed systems; (2) \emph{Uniform memory
access (UMA)} architectures link processors and memory using a bus
interconnect~\citep[Appendix~B.3]{herlihy2020art}.

The cognisance of interconnection architecture is important, 
as some algorithms are not suited for different architectures due to
overheads associated with accessing memory. This study will be
conducted on a \emph{UMA} architecture.

\subsection{Atomic Operations}
% TODO: Mention how CPUs can perform atomics semantically, as opposed to
% performing it in hardware.
% TODO: Find reference for the word size line.
An operation is said to be \emph{atomic}\footnote{Modern CPUs only guarantee
that atomic operations are semantically atomic, meaning that they do not
have to execute atomically on hardware.} if it is impossible to observe the operation in an
intermediate state~\citep{perfbook2021}. On most architectures, atomic
operations may only affect the CPU's word size worth of memory.

\emph{Read-Modify-Write (RMW)} operations read and write to memory
atomically~\citep{perfbook2021}. Most synchronization primitives may be
implemented as RMW operations~\citep[Section~5.6]{herlihy2020art}, and
typically need to be implemented in hardware~\citep[Appendix~B.8]{herlihy2020art}.
\citeauthor{herlihy1991wait} proved that not all synchronization primitives are
equally powerful by associating a \emph{consensus number} (i.e.~the number of
processes that the \emph{consensus problem} can be solved for) to each
synchronization primitive. \emph{Non-atomic} operations are prone to \emph{load} and \emph{store
tearing}~\citep[Section~4.3.4]{perfbook2021}\footnote{The Intel and ARM
architectures guarantee that correctly aligned loads and stores are
atomic~[\citealp{intel2021system},~Section~8.1.1;
\citealp{arm2022architecture},~Chapter~2.2].}.

\citeauthor{scott2013shared} lists a number of RMW operations~\citep[Table~2.2]{scott2013shared}.
\emph{Compare-and-swap} (\emph{CAS}, also known as \emph{Compare and Exchange})
is a ternary atomic operation that requires: A source, destination, and an
update value. If the source and destination are equal, the source is set to the
update value, otherwise, the destination is updated with the source's
value~\citep{intel2021inst}.
Older studies may give different definitions of
CAS~\citetext{\citealp{scott2013shared},~Table~2.2;~\citealp{valois1995datastructures},~Appendix~A},
such that line five is excluded.

\SetNoFillComment
\SetKwComment{Comment}{/* }{ */}
\begin{algorithm}[hbt!]
    \caption{x86 compare-and-swap pseudocode.}\label{alg:cas}
        \KwIn{ptr<T> source, ptr<T> destination, T updateValue}
        \KwOut{boolean success}
        \If{$Source~=~Destination$}
        {
            $Source~\gets~UpdateValue$\;
            \Return{True}\;
        }
        $Destination~\gets~Source$\label{alg:line:sourceneqdest}\;
        \Return{False}\;
\end{algorithm}

\subsection{ABA Problem}
\citeauthor{dechev2010understanding} define the ABA problem as a false positive
execution of a CAS on a shared location~\citep{dechev2010understanding}. The ABA
problem may occur when the source's memory address has been changed from $A$ to
$B$, and back to $A$, bearing in mind that $A$ has been freed and immediately
allocated a logically distinct value, between the point in time when the
destination was loaded and the CAS was executed~\citep{dechev2010understanding}.

% [x] Caching is needed to avoid accessing slow memory by copying data into faster memory hardware
% [x] There are different levels cache, there is the register, L1, L2, and L3
% As cache levels move towards the register, access times get faster and cache sizes get smaller since they are more expensive.
% [x] CPU cache lines are used as a unit of transfer between different caches, on most architectures, a cache line is 64 bytes in size.
% [x] Mention what false sharing is and why it should be avoided.
% [] Mention how false sharing impacted this study's performance before being fixed.

\subsection{Progress Conditions}
A \emph{progress condition} is a \emph{liveness property} that describes how,
and when a number of threads are able to make \emph{forward
progress}~\citep[Section~3.2]{scott2013shared}.

A concurrent method is said to be \emph{blocking} if there is some reachable
state in the system where a thread cannot make progress until some other thread
takes action~\citep[Section~3.2]{scott2013shared}. Algorithms that rely upon
mutual exclusion are considered to be
blocking~\citep[Section~3.2]{scott2013shared}, however, non-lock-based
algorithms may also be blocking~\citep{mellor1987concurrent}.

A concurrent method is said to be \emph{non-blocking} if there is no reachable
state in the system where a thread cannot make
progress~\cite[Section~3.2]{scott2013shared}. The non-blocking property is
desired due to its immunity to random delays. % TODO: Find citation

The following are a subset of non-blocking progress conditions, ordered by the
strictness of the property (starting from the most relaxed) and the complexity of
implementation (starting from the simplest):

\begin{enumerate}
\item \emph{Obstruction-Freedom}: A method is obstruction-free\textemdash if from any
point after which it executes in isolation\textemdash it finishes in a finite number of
steps~\citep[Section~3.8.3]{herlihy2020art};
\item \emph{Lock-Freedom}: A method is lock-free\textemdash if \textemdash some method call finishes
in a finite number of steps~\citep[Section~3.8.2]{herlihy2020art};
\item \emph{Wait-Freedom}: A method is wait-free\textemdash if \textemdash every call finishes its
execution in a finite number of steps~\citep[Section~3.8.1]{herlihy2020art}.
\end{enumerate}

Stronger guarantees do not necessarily equate to higher throughput, as the
overhead and complexity of the implementation tends to be positively
correlated to the strength of the guarantee.

\subsection{Correctness Conditions}
\label{sec:correctness_conditions}
\emph{Correctness conditions} describe the pre and post conditions for a concurrent
object's operations~\citep{herlihy2020art}. Such conditions decide
whether a concurrent history is legal \citep{herlihy1990linearizability}.
Correctness conditions are based on two requirements: (1) When an
operation takes effect, and (2) how the order of non-concurrent operations
should be preserved \citep{herlihy1990linearizability}.

A method is said to \emph{take effect} when the effects of its method call is
seen by other method calls~\citep[Section~3.4.1]{herlihy2020art}. 

% Find a formal definition of "takes effect"
\emph{Concurrent systems} may be modelled as a \emph{history}, which is a
sequence of events, where each event is either an \emph{invocation} or a
\emph{response}~\citep[Section~3.6.1]{herlihy2020art}. 

A history is said to be \emph{legal} with respect to a correctness property if
it is \emph{equivalent} to a history that respects said correctness property.

\citeauthor{lamport1979make} defines the \emph{sequential consistency}
correctness condition as a history in which the result of an execution is the
same as if the operations had been executed in \emph{program
order}~\citep{lamport1979make}.

A \emph{linearizable} concurrent computation gives the illusion that a method
call takes effect instantaneously some time between the method's invocation and
response; the point in time when the method takes effect is also known as the
\emph{linearizability point} \citep{herlihy2020art,herlihy1990linearizability}.
Processors perceive linearizable operations in a \emph{total, and real-time
order}, where overlapping method calls take effect in a non-deterministic
order~\citep[Section~3.6.2]{herlihy2020art}.


% [] What is a memory consistency model?
% [] Sequential Consistency
% [] Weak Consistency
% [] ARM is weakly consistent when writing to normal memory.
% [] Xbox360 CPUs are sequentially consistent as they reads and writes are in order (be careful, as in order might not be sequential consistence).
% [] Modern Intel CPUs use a variation of sequential consistency known as program-ordered-consistency

\subsection{Memory Consistency Models}
\emph{Memory Consistency Models} (referred to as \emph{memory models} henceforth) are correctness properties that describe how
threads perceive the order of other threads' effects on shared
memory~\citep[Section~3.7]{herlihy2020art}. Similarly to the correctness
conditions described in section~\ref{sec:correctness_conditions}, memory
models vary in strength; as the model used becomes more relaxed, more aggressive
optimizations may be used, increasing performance at the cost of
programmability and portability~\citep{gharachorloo1996consistency}.

\emph{Relaxed memory models} such as \emph{weakly ordered models}\footnote{A number of ARM processors tend to be weakly ordered when interacting with main memory~\citep[Section~A3.5.5]{arm2022architecture}} offer little to no
constraints with respect to the ordering of reads and writes, requiring the
explicit use of memory barriers for a finer grained control over the ordering~\citep{gharachorloo1996consistency}. \citeauthor{gharachorloo1996consistency} identifies relaxed
memory models as \emph{system-centric models}, since they tend to favour
performance, as opposed to a programmer's intuition about shared-memory
(sequential consistency)~\citep{gharachorloo1996consistency}.

Memory models\textemdash such as sequentially consistent models\textemdash are considered to be
\emph{stron\-gly ordered}. Strongly ordered systems tend to
heavily restrict the reordering of instructions, giving a simpler and
higher-level interface between memory and the programmer. 

\subsection{Mutual Exclusion}
\emph{Mutual exclusion} (also known as a \emph{mutex}, or a \emph{lock}) is a
synchronization protocol that \emph{serializes} the concurrent execution of
code inside \emph{critical sections}. A \emph{spin-lock} is a variant of a
mutex, that \emph{busy-waiting}, and can be released by any thread.

Every mutex has to decide what actions to take when a critical section has
already been acquired; the three categories said action can fall under are: 
\begin{itemize}
\item \emph{Blocking}:~Voluntarily yields to the CPU if the mutex is acquired.
Blocking is not suitable for small critical sections\footnote{A critical
section is small if it takes hundreds of nanoseconds or micro seconds to
execute.}, as context-switches typically take tens of milliseconds.
\item \emph{Busy-Waiting (spinning)}:~Polls an area in memory containing the
state of a mutex. \emph{Busy-waiting} is suited for small critical sections, as
contention caused by polling may lead to degraded performance;
\item \emph{Hybrid}:~Utilizes a mix of \emph{blocking} and \emph{busy-waiting},
forgoing the inefficiencies associated with both methods.
\end{itemize}

Algorithms reliant on mutual exclusion are inherently
blocking~\citep[Section~3.8]{herlihy2020art}; \emph{starvation freedom} and
\emph{deadlock freedom} are some examples of correctness conditions unique to
\emph{blocking} algorithms.

% TODO: Add a subsection on fairness

\section{Queues}
\citeauthor{knuth1968art} defines a queue as a linear list for which all
insertions occur at one end of the list and deletions occur at the
other~\citep{knuth1968art}. A queue comes with two operations, enqueue (places
an item at the head of the queue) and dequeue (removes and returns an item at
the tail of the queue), following First in First out ordering (FIFO).

\subsection{Concurrent Queues}
Concurrent queues are types queues that remain consistent and correct when
accessed simultaneously through a number of threads. Non-concurrent\textemdash or
ill-synchronized queues\textemdash typically become inconsistent after being
accessed through multiple threads, leading
to hard-to-find bugs~\citep{yahav2003automatically}. Concurrent queues are often the basis of scheduling
algorithms~\citep{debattista2002high} and many other concurrent
algorithms~\citep{yahav2003automatically}. Linearizability is the de-facto correctness condition
for non-blocking queues, as the semantics of the queue's
operations remain unchanged \citep{mellor1987concurrent, valois1995datastructures}.
% TODO: Add "Section 2.2.2" to valois1995datastructures reference

The multiplicity of producers and
consumers that can simultaneously interact with a concurrent queue is used as a
taxonomy to categorize queues;
furthermore, the underlying data structure used to implement a queue (such as a
circular buffer, linked list, or both) affects if the queue is of \emph{fixed
capacity (bounded)} or not \emph{(unbounded)}. 

This study focuses on \emph{unbounded, MPMC, blocking and
non-blocking queues}, with chapter \ref{chap:lit_review} covering the evaluated
queues in further detail.

\begin{table*}[h]\centering
\caption{Possible configurations in the Producer-Consumer taxonomy}\label{tab:producer_consumer}
    \begin{tabular}{lll}
        \hline
        Label & Description & Reference \\ \hline
        SPSC & Single-Producer/Single-Consumer & \citep{aldinucci2012efficient} \\
        SPMC & Single-Producer/Multi-Consumer & \citep{arnautov2017ffq} \\
        MPSC & Multi-Producer/Single-Consumer & \\
        MPMC & Multi-Producer/Multi-Consumers & \citep{michael1996simple,valois1994queues,hoffman2007baskets}\\ \hline
    \end{tabular}        
\end{table*}

\input{chap3/lit_review_related_work.tex}