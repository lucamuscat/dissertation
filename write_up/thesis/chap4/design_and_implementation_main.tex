\chapter{Design \& Implementation}
This chapter focuses on describing and justifying design decisions,
implementation details (research objective~\textbf{O1}),
validation (research objective~\textbf{O2}), and measures
taken to mitigate sources of error.

\section{Relevant Software Versions}
Source code was compiled in the C programming language, using the clang
[version] compiler with flags `-O3 -march=native`\footnote{As opposed to the
gcc compiler, clang emits the \emph{cmpxchg16b} instruction without any extra flags.}. A number of researchers
utilize languages with heavy runtimes~\citep{kogan2011wait}, such as Java's JVM,
in order to shift the responsibility of memory management to the garbage
collectors, instead of the algorithm itself (making the C language's
light-weight runtime a reasonable choice).

Arguably, implementations of non-blocking algorithms in lock-based, garbage
collected languages are not truly non-blocking. \citeauthor{fog2020optimizing} argues that heavy-runtimes
are to be avoided, as the cost of operating the runtime is
typically greater than executing the algorithm
itself~\citep{fog2020optimizing}.

\emph{PAPI (Performance Application Programming Interface)} was used to measure
the elapsed time in nanoseconds~\citep{terpstra2010papi}. Although an entire library for measuring time 
is overkill, PAPI also provides an interface for measuring and
sampling hardware counters, which was used during the development of this project.


\section{Methodology}

% TODO: Can add more citations to this part; can be taken from the literature
% review once that's done.
Studies introducing novel concurrent
algorithms~\citep{michael1996simple,valois1994queues,kogan2011wait,hoffman2007baskets,yang2016wait}
frequently conduct comparisons between their novel and existing algorithms in
similar fashions. The most common benchmarks for evaluating concurrent queueing
algorithms are the \emph{Pairwise Enqueue Dequeue} and \emph{50\% Enqueue}
benchmark. As to amortize measurement overheads, ten million iterations are
executed amongst $N$ threads ($t_0,~t_1,~\cdots,~t_{N-1}$) as follows: 

\[
    \text{iterations(i)} = 
    \begin{cases}
        \lfloor\frac{10^7}{N}\rfloor, & \text{if}~i < N-1 \\
        10^7 - (\lfloor \frac{10^7}{N} \rfloor \cdot (N-1)), & \text{otherwise}
    \end{cases} \\
\]

for ten times, with the recorded sample being the average of the average
elapsed time from each thread for each repetition.

\subsection{Pairwise Enqueue Dequeue Benchmark}

On an initially empty queue, each thread enqueues an item, executes an
artificial delay (which \citeauthor{michael1996simple} refers to as "other
work"~\citep{michael1996simple}), dequeues an item, and executes another
artificial delay; The queue is destroyed and recreated with every repetition
(i.e. every ten millions iterations). \citeauthor{michael1996simple} uses an
artificial delay to reduce the bias introduced by long runs of queue operations
by a single thread~\citep{michael1996simple}. The number of threads, and length
of the artificial delay are used as the benchmark's independent variables, the
degree of contention is dependent on both the number of threads and the length
of the delay, furthermore, the measured time taken is also dependent on
contention and operating system activities. This benchmark serves the purpose
of simulating a producer-consumer workload, where each thread acts both as a
producer, and a consumer.

\subsection{50\% Enqueue Benchmark}
Each thread either executes an enqueue or a dequeue, with each operation
getting a 50\% chance of executing\footnote{Using a uniform distribution},
followed by an artificial delay, on a queue that is prefilled with 1000
elements. The queue is prefilled with 1000 items to prevent long runs of empty
dequeues (potentially introducing biases). Randomness aids with exercising a
larger number of code paths.

Random numbers are stored as double-precision floating point variables, ranging
from zero to one, furthermore, they are generated before the execution
of the benchmark, as to calculate the number of enqueues (consequently the
amount of memory that needs to be allocated), and to reuse the same random
numbers with every repetition, in order to maintain consistency. 

It is important to note that the 50\% enqueue benchmark will execute at most
half the number of enqueues and dequeues executed as the pairwise
enqueue-dequeue benchmark, as an operation is no longer executing a pair of
enqueues and dequeues, but either one of them.

\subsection{Sources of Error}
Experimental studies must be aware of any threats to the validity of
their results. An \emph{observational error} is the difference between what is
measured, and the true result, which can be caused by \emph{systemic} or
\emph{random errors}. Systemic errors are caused by the inherent inaccuracies
of measurement apparatus; with respect to the framework's
measurements, an example of systemic error may be attributed to the resolution
(granularity) of the clock used to measure elapsed time. The CPU's clock speed is
fixed to its maximum frequency by setting the CPU frequency governor to
\emph{performance} mode (using \emph{cpufreq-set -g performance}) in order to
restrict fluctuations in the CPU's clock speed, which may lead to inaccurate
measurements and delays at the cost of higher energy consumption. Random errors cause inconsistencies in repeated
measurements; variance is
commonly attributed to operating system activities, such as process migration
across cores, and context switching.

Hyperthreading was disabled, as it led to higher rates of cache misses due to each core's cache
having to be split with another thread~\citep{fog2020optimizing}; Turbo boost was
disabled, since it dynamically controls
processor performance according to the CPU's
thermal status~\citep[Section~14.3.3]{intel2021system}; Threads were pinned to
specific cores using thread affinity to mitigate process migration.

The \emph{observer effect} is the impact that the cost of taking measurements
has on the behaviour of the algorithm. Concurrent algorithms are highly
timing-sensitive\footnote{As a testament to the timing-sensitive nature of
concurrent algorithms, the term \emph{heisenbug} has been coined for bugs that
tend to disappear when debugging overheads are
introduced~\cite{perfbook2021}.}, as measurement overheads may influence thread
interleavings, causing the true reality of the algorithm to significantly
differ when under observation. By measuring a significantly large number of
operations, the mean time is derived and used as a sample, amortizing
measurement costs along the way. \citeauthor{aceto2021benchmarking} note that the repeatability of experiments
is affected by data variability~\citep{aceto2021benchmarking}; the ratio of
standard deviation to mean (\emph{coefficient of variation}), $\text{CV} =
\frac{\sigma}{\bar{x}} \cdot 100$ is used to quantify the repeatability of
each experiment.
\section{Benchmarking Framework API}

\section{Design Decisions}
This study focuses on four concurrent queueing algorithms:
\citeauthor{michael1996simple}'s Lock-Free Queue~\citep{michael1996simple};
~\citeauthor{michael1996simple}'s Two-Lock Concurrent
Queue~\citep{michael1996simple} (protected by a \emph{test-and-test-and-set-lock}~\citep{mellor1991algorithms}); \citeauthor{hoffman2007baskets}'s Baskets
Queue~\citep{hoffman2007baskets}; and \citeauthor{valois1994queues}' Lock-Free
Queue~\citep{valois1994queues}. Keen-eyed readers may realize that the chosen
queues are unbounded and MPMC.

\subsection{ABA Avoidance}

\emph{Version tagging} is used for ABA avoidance (not to be confused with
ABA-Freedom)~\citep{dechev2010understanding}:
A \emph{counter} (also known as a \emph{tag}) is attached to a pointer, which increments with
each successful CAS. Comparisons between pointers that have the same address,
but do not match in tag value are expected to fail, as differing tag values hint
at the possibility of a change in logical data. Counters may overflow, leading to tags wrapping back
to their initial value, allowing for ABA violations to still occur (however,
the chances are practically close to zero).

% When should version counting with DWCAS be used
For systems with a significant number of processors (for instance multi-CPU NUMA
architectures), larger counters are required  to safely avoid the ABA problem.
The \emph{Double-Width Compare-and-Swap} instruction (\emph{DWCAS}, \emph{CMPXCHG16b} on
\emph{Intel x86\_64}) may be used to attach a 64 bit counter to a pointer; Intel
systems require that the pointer and counter are aligned to a 16 byte boundary.

\begin{lstlisting}[language=C,caption={Struct aligned to 16 bytes, as required by the DWCAS instruction.}]
struct tagged_ptr{
    void* ptr;
    uint64_t tag;
} __attribute__((aligned(16))) tagged_ptr;
\end{lstlisting}

% Introduce pointer packing/tagged pointers
Alternatively, systems that do not provide the DWCAS instruction, suffer from
an unacceptable degradation in performance due to DWCAS, or do not allow for
the modification of pointers may employ a single-word CAS, by attaching a
counter to the pointer itself (known as \emph{pointer packing}, or \emph{tagged
pointers}) through the use of masking and shifting. Intel x86\_64 architectures supports 64 bit pointers, however, the
architecture only makes use of the least significant 48 bits (known as the
\emph{linear address})~\citep[Section~3.3.7.1]{intel2021system}, allowing for
the most significant 16 bits to be used as a counter.

% Explain pointer packing flaws
Pointer packing is flawed: The range of values a 16 bit counter can hold ranges
from zero to $2^{17}-1$ (which may easily overflow); pointer packing is not
portable, as there is no guarantee that the endianess, or the linear address
size will remain the same; pointers may only be dereferenced in their canonical
form, meaning that the most significant 16 bits either need to be set to zero
(user-space pointer) or one (kernel-space pointer), as canonical faults will be
thrown when dereferencing a pointer.

\subsection{Nanosecond Accurate Delay}
% High-resolution timers typically tick at a granularity of microseconds
% \textbf{(find a citation)}, leaving considerable slack between nanosecond
% delays and the elapsed time in reality. 
% The benchmarking framework relies on
% nanosecond-accurate ($10^{-9} seconds$) delays as a means of controlling
% contention. 
Delays of arbitrary lengths can be placed between operations, in order to
control contention~\citep{valois1994queues}. High-resolution timers may be accurate up
to a number of microseconds ($10^{-6}$ seconds), requiring a custom delay
function for nanosecond accuracy ($10^{-9}$ seconds). With respect to CPUs,
time may be described in different dimensions,
where the number of CPU cycles can be extrapolated from wall-clock time and the
CPU's clock speed, allowing for an instruction, with a known latency, to be
executed as many times as required to consume an arbitrary number of
time. Unfortunately, extrapolating CPU cycles from wall-clock time
requires a fixed CPU clock speed, hurting the validity of results in cases
where CPU clock speeds are highly unstable.

% Drawing inspiration from \citeauthor{intel2013lockscaling}'s~lock
% scaling analysis on
% Xeon\textregistered~processors~\citep{intel2013lockscaling}, inter-arrival
% times are on the scale of nanoseconds.

Similar to \citeauthor{intel2018spinloop}'s
\emph{Architecture Agnostic Spin-Wait Loops}~\citep{intel2018spinloop} and
\emph{pollDelay} function~\citep[Example~2.3]{intelmanualoptimization}, a
nanosecond delay can be created by constantly polling the CPU's timestamp
counter, until the extrapolated number of cycles are
consumed~\cite{ramalhete2019delay}.

\begin{algorithm}
    \caption{Delay implemented through polling the CPU's timestamp counter.}\label{alg:delay}
    \KwIn{uint64\_t time\_ns}
            \Comment{Pre-calulate to prevent inaccuracies.}
            $cycles \gets time\_ns \cdot cpu\_frequency\_ghz$ \\
            $start \gets read\_timestamp()$ \\
            \Repeat{$stop - start < cycles$}
            {
                $stop \gets read\_timestamp()$
            }
\end{algorithm}

The delay's accuracy is significantly improved through calibration, where the
delay function is put through a trial run, and the number of cycles to be
executed are modified according to the error between the delay's average time
elapsed, and the expected delay.

\subsection{Test-and-Test-and-Set Lock with Exponential Delay}
\citeauthor{rudolph1984dynamic} propose an alternative to the test and set
(TAS) spinning method called test-and-test-and-set
(TTAS)~\citep{rudolph1984dynamic}. The steps TTAS takes to reduce cache line
invalidations caused by indiscriminate TAS instructions are two-fold: (1) Read
the polled flag until a falsey value has been read (lock has been released);
(2) Acquire the lock using the test-and-set instruction, going back to the
first step if the test-and-set operation returns a truthy value (another thread
has acquired the lock between steps one and two). 

Exponential back-off is needed in order to control
contention~\citep{valois1994queues}. \citeauthor{anderson1990performance}
suggests a bound on the delay, as uncontended processors that back-off a number
of times will take longer to acquire a lock; furthermore, unfairness is further
exacerbated when using exponential delay~\citep[Section~7.4]{herlihy2020art}.
\citeauthor{intelmanualoptimization}'s \emph{Contended Locks with Increasing
Back-off Example} was adapted in the lock's implementation~\citep{intelmanualoptimization}.

\begin{lstlisting}[language=C,caption={Test-and-Test-and-Set Lock Struct.}]
typedef struct ttas_lock_t
{
    // Do not share the cache line
    __attribute__((aligned(64))) atomic_bool busy;
} ttas_lock_t;

\end{lstlisting}

\begin{algorithm}
    \caption{Pseudocode for Acquiring a Test-and-Test-and-Set
    Lock.}\label{alg:ttas_acquire}
        \Comment{Acquire}
        $\text{mask} \gets 1$
        $\text{max\_backoff} \gets 64$
        \Repeat{\text{atomic\_exchange\_explicit(lock.busy,
        1,memory\_order\_acquire)}}
            {
            
            \While{atomic\_load\_explicit(lock.busy, memory\_order\_acquire)}{
                $i \gets \text{mask}$
                \For{$i \rightarrow 0$}
                {
                    PAUSE \Comment{Reduces contention from spin-waiting}
                }
                \eIf{$\text{mask} < \text{max\_backoff}$}
                {
                    $\text{mask} \gets \text{mask} \cdot 2$
                }
                {
                    $\text{mask} \gets \text{max\_backoff}$
                }
            }
        }
        \Comment{Start Critical Section}
        \Comment{Critical Section}
        \Comment{End Critical Section}
        atomic\_store\_explicit(lock.busy,0,memory\_order\_release)\Comment{Release}
\end{algorithm}

\subsection{Memory Management}
Due to limited time, the dynamic memory management of non-blocking
data-structures is omitted, as it is an actively researched field which can fit
an entire dissertation in of itself~\citep{valois1995datastructures,michael2004hazard}.

Memory is allocated before each benchmark, as to avoid degrading
non-blocking progress conditions through the use of lock-based memory
allocation. Pre-allocating memory prevents the ABA problem (as memory addresses
may no longer be reused)~\citep{dechev2010understanding}, removes the overhead
of memory allocation in measurements, and forgoes the need of memory management
schemes, as memory is released at the end of the benchmark.

\section{Performance Considerations}