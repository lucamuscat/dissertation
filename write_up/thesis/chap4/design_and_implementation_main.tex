\chapter{Design \& Implementation}
This chapter marks out the design and implementation of the benchmarking
framework (research objective~\textbf{O1}). A discussion on the steps taken to validate the
present study's results (research objective~\textbf{O2}), together
with efforts to sources of error are discussed. The queues implemented in this
study (as per research objective~\textbf{O3}) are discussed.

\section{Relevant Software Versions}
Large runtime environments, such as Java's JVM, are regularly used in
concurrent programming, as the responsibility of memory management falls on the
garbage collector~\citep{kogan2011wait}. \citeauthor{fog2020optimizing} notes
that large runtimes are typically more resource-consuming than the executed
algorithms~\citep{fog2020optimizing}. Arguably, implementations of non-blocking
algorithms in lock-based, garbage-collected environments are not truly
non-blocking. The study is implemented using the C programming language, since
it uses a relatively light runtime without garbage collection. The clang
\emph{version 10.0.0-4ubuntu1} compiler, with \emph{-O3 -march=native}
optimisation flags are used for compilation. An \emph{Intel Core i7 6700HQ
(Skylake)}, with four cores and a base frequency of $2.60\text{GHz}$, together
with \textbf{16GB} of RAM is used throughout the rest of this study.

\emph{PAPI (Performance Application Programming Interface)} is used to measure
elapsed time in nanoseconds~\citep{terpstra2010papi}. Although an entire library for measuring time 
may seem excessive, PAPI also provides an interface for
sampling hardware counters, which can be used for debugging purposes.

\section{Methodology\label{sec:design_and_implementation_methodology}}

% TODO: Can add more citations to this part; can be taken from the literature
% review once that's done.
Researchers introducing novel concurrent
algorithms~\citep{michael1996simple,valois1994queues,kogan2011wait,hoffman2007baskets,yang2016wait}
frequently compare their contributions with existing algorithms. The
\emph{Pairwise Enqueue Dequeue} and \emph{50\% Enqueue}
benchmarks are considered to be standard benchmarking procedures for concurrent
queueing algorithms. Ten million operations are
executed amongst $N$ threads ($t_0,~t_1,~\cdots,~t_{N-1}$) as follows: 

\[
    \text{iterations(i)} = 
    \begin{cases}
        \lfloor\frac{10^7}{N}\rfloor, & \text{if}~i < N-1 \\
        10^7 - (\lfloor \frac{10^7}{N} \rfloor \cdot (N-1)), & \text{otherwise}
    \end{cases} \\
\]

The average of each thread's average elapsed time for ten repetitions,
excluding time spent in artificial delays is used as the recorded sample.

\subsection{Pairwise Benchmark}

On an initially empty queue, each thread enqueues an item, executes an
artificial delay (which \citeauthor{michael1996simple} refer to as ``other
work''~\citep{michael1996simple}), dequeues an item, and executes another
artificial delay. The queue is destroyed and recreated with every repetition
(i.e.every ten millions iterations). \citeauthor{michael1996simple} use an
artificial delay to reduce the bias introduced by long runs of queue operations
by a single thread~\citep{michael1996simple}. Quantity of threads, and the length
of the artificial delay are the experiment's independent variables.
The degree of contention is dependent on both the number of threads and the length
of delay; furthermore, the time measured is also dependent on
contention and operating system activities. This benchmark serves the purpose
of simulating a producer-consumer workload, where each thread acts both as a
producer and a consumer.

\subsection{50\% Enqueue Benchmark}
Drawing from a uniform distribution, each thread is equally probable to execute
an enqueue or a dequeue, followed by an artificial delay. The queue prefilled
with 1000 items to prevent long runs of empty dequeues (potentially introducing
biases). Randomness aids with exercising a larger breadth of code paths.

Random numbers are stored as double-precision floating point variables, ranging
from zero to one; furthermore, they are generated in the benchmark's setup
phase as to calculate the number of enqueues (consequently the
amount of memory that needs to be allocated), and to reuse the same random
numbers with every repetition, maintaining consistency across measurements. 

The \emph{50\% enqueue benchmark} executes at most
half the number of enqueues and dequeues as the \emph{pairwise
benchmark}, as an iteration no longer consists of a pair of
enqueues and dequeues, but either one of them.

\subsection{Sources of Error}
Researchers must be aware of any threats to the validity of
their results. 
An \emph{observational error} is the difference between what is
measured\textemdash and the true result\textemdash which can be caused by
\emph{systemic} or \emph{random errors}~\citep{lelbach2015cppbenchmarking}.
\emph{Systemic errors} are caused by the inherent inaccuracies of measurement
apparatus. 
With respect to the framework's measurements, a source of system errors is the
resolution (granularity) of the clock used to measure elapsed
time~\citep{lelbach2015cppbenchmarking}. 
The CPU's clock speed is fixed at its maximum frequency by setting the CPU
frequency governor to \emph{performance} mode (using the `\emph{cpufreq-set -g
performance}' command) in order to restrict fluctuations in the CPU's clock
speed, which may lead to inaccurate measurements and delays at the cost of
higher energy consumption. Random errors cause inconsistencies in repeated
measurements.
Variance is commonly attributed to operating system activities, such as process
migration across cores, and context-switching.

Hyperthreading is disabled as it leads to higher rates of cache misses due to
the each core's cache being split with an extra
thread~\citep{fog2020optimizing}.
Turbo boost is disabled, since it dynamically controls processor performance
according to the CPU's thermal status~\citep[Section~14.3.3]{intel2021system}.
Threads are pinned to specific cores using thread affinity to reduce \emph{process
migration}.

The \emph{observer effect} describes the phenomena where the behaviour of an
observed experiment deviates
from its true form under no observation. Concurrent algorithms are
highly timing-sensitive\footnote{As a testament to the timing-sensitive nature
of concurrent algorithms, the term \emph{heisenbug} has been coined for bugs
that tend to disappear when debugging overheads are
introduced~\cite{perfbook2021}.}, as measurement overheads may influence the
interleaving of threads. By measuring a significantly large
number of operations, the mean time is derived and used as a sample, amortizing
measurement costs. \citeauthor{aceto2021benchmarking} note that
data variability affects the repeatability of experiments~\citep{aceto2021benchmarking}; the ratio of standard deviation to
mean (\emph{coefficient of variation}), $\text{CV} = \frac{\sigma}{\bar{x}}
\cdot 100$ is used to quantify the repeatability of each experiment.

% TODO: Don't forget to mention the modifications made to the baskets queue.
\section{Design Decisions\label{sec:design_and_implementation_design_decisions}}
This study focuses on four concurrent queueing algorithms:
\citeauthor{michael1996simple}'s Lock-Free Queue~\citep{michael1996simple};
~\citeauthor{michael1996simple}'s Two-Lock Concurrent
Queue~\citep{michael1996simple} (protected by a \emph{test-and-test-and-set-lock}~\citep{mellor1991algorithms}); \citeauthor{hoffman2007baskets}'s Baskets
Queue~\citep{hoffman2007baskets}; and \citeauthor{valois1994queues}' Lock-Free
Queue~\citep{valois1994queues}. Keen-eyed readers may observe that the chosen
queues are \emph{unbounded} and \emph{MPMC}.

\subsection{ABA Avoidance}

\emph{Version tagging} is used for ABA avoidance (not to be confused with
ABA-Freedom)~\citep{dechev2010understanding}:
A \emph{counter} (also known as a \emph{tag}) is attached to a pointer, which increments with
each successful CAS. Comparisons between pointers that have the same address,
but do not match in tag value are expected to fail, as differing tag values hint
at the possibility of a change in logical data. Counters may overflow, leading to tags wrapping back
to their initial value, allowing for ABA violations to still occur (the chances
of which are virtually nil).

% When should version counting with DWCAS be used
For systems with a significant number of processors (for instance multi-CPU
NUMA architectures), larger counters are required  to safely avoid the ABA
problem. The \emph{Double-Width Compare-and-Swap} instruction (\emph{DWCAS},
\emph{CMPXCHG16b} on \emph{Intel x86\_64}) may be used to attach a 64 bit
counter to a pointer. 
Intel systems require that the pointer and counter are aligned to a 16 byte
boundary.

\begin{lstlisting}[language=C,caption={Struct aligned to 16 bytes, as required by the DWCAS instruction.}]
struct tagged_ptr{
    void* ptr;
    uint64_t tag;
} __attribute__((aligned(16))) tagged_ptr;
\end{lstlisting}

% Introduce pointer packing/tagged pointers
Alternatively, systems that do not provide the DWCAS instruction, suffer from
an unacceptable degradation in performance due to DWCAS, or do not allow for
the modification of pointers may employ a single-word CAS, by attaching a
counter to the pointer itself (known as \emph{pointer packing}, or \emph{tagged
pointers}) through the use of masking and shifting. Intel x86\_64 architectures support 64 bit pointers, however, the
architecture only makes use of the least significant 48 bits (known as the
\emph{linear address})~\citep[Section~3.3.7.1]{intel2021system}, allowing for
the most significant 16 bits to be used as a counter.

% Explain pointer packing flaws
Pointer packing is flawed: 
The range of values a 16 bit counter can hold ranges from zero to $2^{17}-1$
(which may easily overflow).
Pointer packing is not portable, as there is no guarantee that the endianess,
or the linear address size will remain the same. 
Pointers may only be dereferenced in their canonical form, meaning that the
most significant 16 bits either need to be set to zero (user-space pointer) or
one (kernel-space pointer), as canonical faults are thrown when
dereferencing a non-canonical pointer.

\subsection{Nanosecond Accurate Delay}
% High-resolution timers typically tick at a granularity of microseconds
% \textbf{(find a citation)}, leaving considerable slack between nanosecond
% delays and the elapsed time in reality. 
% The benchmarking framework relies on
% nanosecond-accurate ($10^{-9} seconds$) delays as a means of controlling
% contention. 
Delays of arbitrary lengths can be placed between operations, in order to
control contention~\citep{valois1994queues}. High-resolution timers are
accurate up to a number of microseconds ($10^{-6}$ seconds), making it
unsuitable for nanosecond accurate measurements ($10^{-9}$ seconds). 
With respect to CPUs, time may be described in different dimensions, where the
number of CPU cycles can be extrapolated from wall-clock time and the CPU's
clock speed, allowing for an instruction\textemdash with a known
latency\textemdash to be executed as many times as required to consume an
arbitrary number of time. Unfortunately, extrapolating CPU cycles from
wall-clock time requires a fixed CPU clock speed, hurting the validity of
results in cases where CPU clock speeds are highly unstable.

% Drawing inspiration from \citeauthor{intel2013lockscaling}'s~lock
% scaling analysis on
% Xeon\textregistered~processors~\citep{intel2013lockscaling}, inter-arrival
% times are on the scale of nanoseconds.

Similar to Intel's
\emph{Architecture Agnostic Spin-Wait Loops}~\citep{intel2018spinloop} and
\emph{pollDelay} function~\citep[Example~2.3]{intelmanualoptimization}, a
nanosecond-accurate delay is implemented by constantly polling the CPU's timestamp
counter until the extrapolated number of cycles are
consumed~\cite{ramalhete2019delay}.

\begin{algorithm}
    \caption{Delay implemented through polling the CPU's timestamp counter.}\label{alg:delay}
    \KwIn{uint64\_t time\_ns}
            \Comment{Pre-calculate to prevent inaccuracies.}
            $cycles \gets time\_ns \cdot cpu\_frequency\_ghz$ \\
            $start \gets read\_timestamp()$ \\
            \Repeat{$stop - start < cycles$}
            {
                $stop \gets read\_timestamp()$
            }
\end{algorithm}

The delay's accuracy is significantly improved through calibration, where the
delay function is put through a trial run, and the number of cycles to be
executed are modified according to the error between the delay's average time
elapsed, and the expected delay.

\subsection{Test-and-Test-and-Set Lock with Exponential Delay}
\citeauthor{rudolph1984dynamic} propose an alternative to the \emph{test and set
(TAS)} spinning method called \emph{test-and-test-and-set
(TTAS)}~\citep{rudolph1984dynamic}. The steps \emph{TTAS} takes to reduce cache line
invalidations caused by indiscriminate \emph{TAS} instructions are two-fold: (1) Read
the polled flag until a falsey value has been read (lock has been released);
(2) Acquire the lock using the \emph{TAS} instruction, going back to the
first step if the \emph{TAS} operation returns a truthy value (another thread
has acquired the lock between steps one and two). 

Exponential back-off is needed to control
contention~\citep{valois1994queues}. \citeauthor{anderson1990performance}
suggests a bound on the delay, as uncontended processors that back-off a number
of times will take longer to acquire a lock; furthermore, unfairness
exacerbated under exponential delay~\citep[Section~7.4]{herlihy2020art}.
Intel's \emph{Contended Locks with Increasing
Back-off Example} was adapted in the lock's implementation~\citep{intelmanualoptimization}.

\begin{lstlisting}[language=C,caption={Test-and-Test-and-Set Lock Struct.}]
typedef struct ttas_lock_t
{
    // Do not share the cache line
    __attribute__((aligned(64))) atomic_bool busy;
} ttas_lock_t;

\end{lstlisting}

\begin{algorithm}
    \caption{Pseudocode for Acquiring a Test-and-Test-and-Set
    Lock.}\label{alg:ttas_acquire}
        \Comment{Acquire}
        $\text{mask} \gets 1$
        $\text{max\_backoff} \gets 64$
        \Repeat{\text{atomic\_exchange\_explicit(lock.busy,
        1,memory\_order\_acquire)}}
            {
            
            \While{atomic\_load\_explicit(lock.busy, memory\_order\_acquire)}{
                $i \gets \text{mask}$
                \For{$i \rightarrow 0$}
                {
                    PAUSE \Comment{Reduces contention from spin-waiting}
                }
                \eIf{$\text{mask} < \text{max\_backoff}$}
                {
                    $\text{mask} \gets \text{mask} \cdot 2$
                }
                {
                    $\text{mask} \gets \text{max\_backoff}$
                }
            }
        }
        \Comment{Start Critical Section}
        \Comment{Critical Section}
        \Comment{End Critical Section}
        atomic\_store\_explicit(lock.busy,0,memory\_order\_release)\Comment{Release}
\end{algorithm}

\subsection{Memory Management}
% Is the terminology of dissertation correctly used here? I.e. is is supposed
% to be thesis?
Due to limited time, dynamic memory management of non-blocking data-structures
is omitted, as it is an actively researched field which can single-handedly fit
an entire dissertation ~\citep{valois1995datastructures,michael2004hazard}.

Memory is allocated before each benchmark, as to avoid degrading
non-blocking progress conditions through the use of lock-based memory
allocation. Pre-allocating memory prevents the ABA problem (as memory addresses
may no longer be reused)~\citep{dechev2010understanding}, removes the overhead
of memory allocation in measurements, and forgoes the need of memory management
schemes, as memory is released at the end of the benchmark.

\section{Benchmarking Framework API}
\subsection{Modularity}
New queues and locks may be added to the framework with ease by implementing
interfaces~\ref{sec:queue_interface} and~\ref{sec:lock_interface}. Locks in the
\emph{src/locks} directory are statically linked with every blocking queue
inside \emph{src/queues/blocking}; each blocking
queue variation and non-blocking queue (found in
\emph{src/queues/non-blocking}) are statically linked to every benchmark,
generating a single executable binary for each queue's benchmark. 

\subsection{Lifecycle}
Each benchmark goes through a common lifecycle, which can be categorized into
four phases: 
Firstly, the master thread initializes synchronization primitives
(such as locks and barriers), auxiliary data (such as
sequences of random numbers for the \emph{50\% Enqueue benchmark}), and the
queue to be benchmarked; 
Secondly, $N$ threads are spawned, and are provided
with the necessary arguments;
Thirdly, a contiguous pool of memory proportional
to the number of enqueue operations executed in the warm-up and benchmark are
allocated\footnote{The number of enqueues are extrapolated from the
pre-generated sequence of random numbers in the case a 50\% enqueue benchmark
is used.};
Finally, each thread executes its share of operations and waits until each
thread has finished, in order to safely free up the allocated memory. The
entire cycle is repeated using a newly initialized queue.

\subsection{Auxiliary Data}
Upon the initialization of a queue, up to 64 bytes of memory\textemdash aligned as four
byte integers\textemdash are passed to the queue, and are outputted
the benchmark data. Throughout the study, these 64
bytes store several fine-grained statistics, such the number of failed Compare-And-Swaps,
or the number of times a thread helping mechanism has been invoked.

\subsection{Performance Considerations}
Static linking is utilized as it provides performance benefits over dynamic
linking; \citeauthor{fog2020optimizing} notes that functions inside dynamically
linked libraries take longer to execute, and make less efficient use of code
and data caching~\citep[Section~14.11]{fog2020optimizing}.

The alignment and padding variables reduces false sharing, preventing logically
distinct data from being stored in the same cache line.
Thread-specific data was stored in thread local storage, as to further reduce
false sharing. Struct fields were aligned to 128 bytes, due to
Intel's spatial prefetcher loading data into L2
cache in chunks of 128 bytes~\citep[Section~E.2.5.4]{intelmanualoptimization}.

Threads are synchronized to start at the same time using
barriers, mitigating transient start up effects~\citep{hoffman2007baskets}.
Furthermore, a number of operations are executed before the start of each
benchmark, as to warm up the cache and branch predictor, moving cache misses
due to unpopulated caches and speculative branch mispredictions outside the
recorded benchmarks.

\subsection{Validation\label{sec:design_and_implementation_validation}}
In response to research objective \textbf{O2}, parts of the framework's
components are validated, as to increase confidence in the measurements
obtained. A constant value is enqueued, allowing for the comparison
between the original and dequeued values to test for any corruption caused by race conditions. 

The artificial delay's accuracy is validated through a separate
benchmark where the average delay over ten million operations is recorded several times.
An arbitrary threshold for the coefficient of variance of one percent is used
to validate suitability of the delay; such a test allows for immediate feedback
loops, where changes to the delay's behaviour may be verified in a timely
manner.

The benchmarking framework's auxiliary data stores the number of times an
algorithm visits any specific line of code (visited lines are counted only if
the user instruments the benchmark to do so). Counters act as crude indicators that an
algorithm may not be functioning correctly; for instance, if a counter indicating a
dequeue on an empty queue has occurred for the majority of the benchmark's
iterations, one can deduce that the queue's linked list is corrupted. 

A \emph{Test Driven Development (TDD)} approach was used when implementing the
pointer packing logic, which served the purpose of reducing the chances of
regressions due to code changes or differences in CPU architecture. Atomic
variables are asserted to be lock-free through the
\emph{atomic\_is\_lock\_free} method, which may trigger due to compiler settings,
or unavailability of specific atomic instructions on CPUs. Error checking is
used to prevent the framework from reaching undefined states.