% Layout Author: Luca Muscat
% Website: www.github.com/lucamuscat
% Repository: www.github.com/lucamuscat/LaTex-Layout

\documentclass[a4paper, 12pt, titlepage]{article}

%--------------- Preambles ---------------
\usepackage[none]{hyphenat}
\usepackage{tikz}
\usepackage{aeguill}
\usepackage{setspace}
\usepackage{listings}
\usepackage[newfloat]{minted}
\usepackage[justification=centering]{caption}
\usepackage[margin=0.78in]{geometry}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{hyperref}

\setlength{\footnotesep}{\baselineskip}

\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newenvironment{code}{\captionsetup{type=listing}}{}
% \sourcecode{Directory/NameOfFile}{Caption}{Label}
% #1: Code Snippet
% #2: Caption
% #3: Reference Label
\newcommand{\sourcecode}[3]{
    \begin{code}
      \inputminted[linenos,numbersep=5pt,gobble=0,frame=lines,framesep=2mm,]{python}{#1}
        \caption{#2}
        \label{lst: #3}
    \end{code}
  }

\newcommand{\explainline}[2]{
  \textbf{Line #1} & #2 \\ \\
}

% \image{Scale}{Image Path}{Caption}{Reference Label}
% #1 - Scale
% #2 - Image path
% #3 - Caption
% #4 - Reference Label
\newcommand{\image}[4]{
  \begin{figure}[H]
    \centerline{\includegraphics[scale=#1]{#2}}
    \captionof{figure}{#3}
    \label{fig: #4}
  \end{figure}
}
%---------------  End   ---------------

\begin{document}

%---------------Header---------------
%Good Syntax styles: trac, abap
\title{A comparative study of lock-based & lock-free concurrent queueing algorithms and their performance \\ Progress Report}
\author{Luca Muscat \\ luca.muscat.19@um.edu.mt \\ Supervisor: Prof. Kevin Vella}

\titleformat{\section}{\Large\bfseries\filcenter}{}{0em}{}
\titleformat{\subsection}{\normalsize\bfseries\filcenter}{}{0em}{}
\titleformat{\subsubsection}{\small\bfseries}{}{0em}{}
\maketitle

\pagebreak
\tableofcontents
\pagebreak
%---------------   End  ----------------
\section{Introduction}
%--------------- Content ---------------

% Define wait-free and lock-free
% Define progress and safety
% Liveness vs progress, put backoff in context.
% What are the progress guarantees required for a queue
% Conditions that reflect that the queue is working correctly
% Progress - The state must be advance and progress
% Investigate progress and conditions w.r.t to queues
% Guide the structure of the lit review with sub sections (taxonomy, classify types of entity, in our case queue.)
% Find a good citation for a queue, baby citations, is there someone who classifies a queue? a concurrent one? Start with this
% Condense lines a bit more.
% Start numbering from introduction, not TOC.
% Mention that a subset of the algorithms will be implemented.

\begin{onehalfspacing}
The aim of this thesis is to characterize and compare the performance of blocking \cite{anderson1990performance, herlihy2020art,graunke1990synchronization} and non-blocking \cite{herlihy2020art,valois1995lock} concurrent queueing algorithms. This will be done by gathering different metrics to measure the performance of each queueing algorithm under study under different conditions. The goal of measuring performance under different conditions is to highlight the costs and benefits of each queueing algorithm's progress and safety guarantees (a safety property states that a concurrent algorithm will always remain in a certain state, whilst a progress guarantee determines if a thread is blocking or non blocking \cite{herlihy2020art,mckenney2017parallel}). A section of this thesis will also be dedicated to analyzing different locking algorithms and their performance, the aim of which is also to measure the costs of certain guarantees and the effects that back off algorithms have at mitigating performance degradation under contention \cite{anderson1990performance,graunke1990synchronization,valois1994implementing}. If access to hardware that support certain performance event counters is feasible, this paper will also look into metrics such as data sharing, cache misses and cache line invalidations\cite{intelmanual,intelmanualoptimization,sahelices2009methodology}  to provide more empirical evidence to specific claims.

\section{Literature Review}
Herlihy and Shavit provide a fundamental introduction to parallel computing and shared memory multiprocessor programming in the "Art of Multiprocessor Programming" \cite{herlihy2020art}. The material covered in this text book ranges from intuitive explanations of basic concepts such as mutual exclusion and read modify write operations, all the way to rigorous definitions and proofs of wait-free constructions and concurrent data structures.

``Is Parallel Programming Hard, And, If So, What Can You Do About It '' focuses on shared-memory programming with an emphasis on low level software \cite{mckenney2017parallel}, such as the behaviour of OS kernels and low-level libraries. Mckenney covers concepts such as non-blocking synchronization, memory barriers, read copy update semantics, and other fundamental yet practical concepts required to be able to write multi-threaded programs. McKenney dedicates a section of the book to performance estimation, where different ways errors and interferences may creep up in micro-benchmark readings are highlighted, and suggestions to mitigate said interferences are given.

Preshing shares several of his insights inside the world of lock-free programming through the ``Preshing on Programming'' blog \cite{preshing}. Some entries which are relevant to this dissertation are ``An Introduction to Lock-Free Programming''\cite{introlockfree} where the basics of lock free programming are covered in an easy to digest manner. Flow charts depicting when lock-free programming techniques together with commentary is provided in this article, moreover, concepts such as atomic read-modify-write operations, compare and swap (CAS) loops, and memory ordering are covered in brief detail. 

``Roll your own lightweight mutex''\cite{preshingmutex} covers a simple mutex implementation using a type of semaphore informally known as the `Benaphore'\cite{haikubenaphore}.

``Locks Aren't Slow; Lock Contention Is''\cite{preshinglockcontentionslow} offers insight into the performance of spin locks under contention. Preshing uses a custom implementation of the Mersenne Twister \cite{matsumoto1998mersenne} to simulate a critical section. The workload (ie. the amount of time that the lock is held for) and the number of threads used as variables whilst the lock frequency remained constant. The results show that high levels of contention on a lock is enough to degrade the performance of a parallel solution to the point where a sequential thread will perform better.

Boyd-Wickizer et. al show that non-scalable locks, such as spin locks should not be used in operating systems where contention is hard to control. Several micro-benchmarks were implemented using a spin lock implementation offered by the linux kernel; as the contention increased, the performance of each benchmark dropped drastically. The spin lock was then re-implemented using scalable locks, such as the MCS lock \cite{mellor1991algorithms} and the CLH lock \cite{craig1993building,magnusson1994queue} improved performance on a large number of cores by at least 3.5 times, and in some cases, 16 times.

Segall and Rudolph \cite{rudolph1984dynamic} an alternative to the test and set (TAS) spinning method called test-and-test-and-set (TTAS). TTAS reduces the amount of cache line invalidations caused by TAS spinning by checking if the flag being spinned on has changed before calling TAS.

Anderson offers an improvement to the TTAS lock by implementing several spin locks based on CSMA network protocols \cite{anderson1990performance}. Anderson notes that spin locks using Ethernet's back off protocol tends to perform better under contention than a regular TTAS lock.

Graunke and Thakkar also offer improvements to the TTAS lock by adding a delay after each failed test in order to reduce contention \cite{graunke1990synchronization}. The authors also proposed a novel queuing lock that outperformed all variations of the TAS lock when more than three processors are competing for access.

Mellory Crummey and Scott propose a novel array based lock known as the MCS lock that outperforms both array based queueing locks proposed by Anderson and Graunke et al. \cite{mellor1991algorithms}. The authors also offer insight into existing spin locks such as TAS, TTAS, and the ticket lock, and describe their benefits and their caveats.

Intel's lock scaling analysis on Intel Xeon processors \cite{intelxeonlockscaling} benchmarks the relative contended performance of the Xeon Phi E5-2600 over X5600. The most notable contribution of this paper is the methodology used to for the benchmarks. The authors claim that a microbenchmark that aims to study lock performance does not reflect behaviour on real software if the study does not factor in the length of the simulated critical section and the frequency of locking. The paper reaches the conclusion that when predictable performance is desired, the locking algorithm requires reasonably long critical sections and re-entry times.

Sahelices et al. offer a new methodology for tuning performance and critical sections inside of parallel programs \cite{sahelices2009methodology}. Critical sections were characterized by lock contention and degree of data sharing, allowing the authors to identify a number of inefficiencies caused by data sharing patterns and data layouts. Interestingly, the benchmarks were conducted on a multiprocessor simulator called RSIM, that allowed the authors to take fine grained and accurate statistics.

Gregg describes several performance 'Anti-Methodologies' and 'Methodologies' \cite{methodologygregg}. Anti-Methodologies are methods of benchmarking performance that do not lead to accurate or correct results. This is an article that anyone in the field of performance analysis should read, as it reveals methodical and structured methods of carrying out performance analysis.

Intel's optimization reference manual \cite{intelmanualoptimization} suggests several potential metrics that may be derived from specific hardware counters. Notably, equations for calculating bus utilization, L2 Modified Lines Eviction Rate, and Modified Data Sharing Ratio are all provided.


Valois covered multiple lock-free queue implementations that utilized linked lists, noting that most of the algorithms could be implemented using the Compare and Swap atomic primitive. Valois noted that CAS based algorithms suffer from the ABA problem. \cite{valois1994implementing}

Valois proposed the safe read protocol, which was a solution to the ABA problem that did not require stronger versions of CAS (such as DCAS) primitive \cite{valois1994implementing, valois1995lock}. A new lock free queue that made use of arrays and CAS was also proposed. The novel lock-free queue outperformed most lock-based queues both sequentially and under contention. Valois measured the performance of each lock-free queue with and without protection against the ABA problem, noting that even with the overhead, lock-free queues are still competitive with their lock-based counterparts.

Most notably, Valois measured the performance of each queue affected by random delay. Valois did this by simulating a delay of a random amount of cycles following an exponential distribution with a mean of 1000 cycles and a 10\% chance of occurring. The results showed that the response time of the lock-free implementations were approximately 17\% of that of the spin lock protected queues, highlighting the superiority of lock-based algorithms. The massive difference in performance was attributed to the blocking nature of the spin lock, noting that a delayed spin lock not only affected the current operation, but also the other pending critical sections.

Valois also concluded that spin lock algorithms require some type of back off mechanism in order to decrease the degraded performance caused due to interconnect saturation.

Performance analysis was conducted on a parallel architecture simulator called Proteus. The simulator was a work around to the infeasability of obtaining the necessary hardware and resources needed to obtain readings. Further exasperating this issue, not all machines used architectures that supported CAS primitives; most machines did not have good instrumentation facilities either. Porting code over to other architectures would have proved to be very time consuming \cite{valois1994implementing, valois1995lock}. 

Fortunately, these are mostly issues of the past, as hardware is more accessible and instrumentation facilities have matured over the last few decades.

Kogan et. al propose a novel and efficient wait-free queue that is on based link-lists and thread helping (ie. threads helping other threads by completing their intended operation for them)\cite{kogan2011wait}. Similar to the filter and the bakery locks, the queue is required to read and write to n distinct locations, where n is the maximum number of concurrent threads\cite{herlihy2020art}.
The proposed wait-free queue assumes that a garbage collector is responsible for preventing the ABA problem. Since a wait-free garbage collector does not exist, Kogan et al propose that the hazard pointer technique could be used to solve the ABA problem.

Performance of each queue mentioned in the paper was collected using the following methodology:
\begin{enumerate}
  \item enqueue-dequeue pairs: Starting with an empty queue, each thread iteratively performs an enqueue followed by a dequeue.
  \item 50\% enqueues: The queue is initialized with 1000 elements, each thread does a 'coin toss' to decide
\end{enumerate}

Shavit et. al introduce a novel lock-free queue using a technique known as elimination \cite{moir2005using}. Simply put, elimination works by allowing opposing operations such as enqueues and dequeues to exchange values without synchronizing on a shared data structure \cite{shavit1997elimination, moir2005using}. The results obtained suggests that as levels of concurrency increase, so does the throughput of the proposed data structure.

Fog and McKenney offer insights into how to reduce the number of errors and interventions when micro-benchmarking \cite{fog1996optimizing,fog2020optimizing, mckenney2017parallel}. Some measures to prevent errors are: keeping the CPU clock frequency stable (Stinner described methods of configuring and reading multiple CPU parameters \cite{stinnerpstate}), ignoring the first few iterations due to the cache and the branch predictor being cold\cite{fog1996optimizing}, avoiding symmetric multi-threading (aka. hyper-threading) \cite{fog2020optimizing} and detecting kernel interferences \cite[Chapter~11.7]{mckenney2017parallel}.

\section{Goals}
\subsection{Current Progress}
Different types of locks, such as the TAS lock, TTAS lock\cite{anderson1990performance, graunke1990synchronization,herlihy2020art,mellor1991algorithms}, peterson lock, and the filter lock \cite{herlihy2020art} have been implemented together with a testing environment that aids in the rapid development of the code base. The tests created so far are:
\begin{enumerate}
  \item Counting Test: A shared variable is protected by each lock and incremented a number of times. If the shared variable does not match the expected value, the test will fail. The goal of this test is to detect race conditions. Although it is not an exhaustive test by any means, it is still able to detect faulty implementations and race conditions.
  \item Sequential Latency Test: The term sequential latency refers to the amount of time for a process to execute a single operation in the absence of any contention \cite[Section~6.3.1]{valois1995lock}\cite[Section~4.3, Table~1]{mellor1991algorithms}. According to Valois, sequential latency serves as a good indicator of the number of steps taken in an operation \cite[Section~6.3.1]{valois1995lock}. Figure \ref{fig: seq_lat} shows the current findings.
\end{enumerate}

\image{1}{./sequential_latency.png}{Sequential latency of each lock depending on the number of cores. The filter lock is a starvation-free lock algorithm (starvation freedom implies deadlock freedom)\cite{herlihy2020art}. Any deadlock free algorithm has a linear bound to the number of writes and reads necessary \cite[Section~2.8]{herlihy2020art}, accurately reflecting results obtained. The results may be slightly skewed, as they were obtained on a virtualized system. Interference detection mechanisms are yet to be implemented and a properly configured machine (correct power settings, removal of competing processes) is yet to be setup.}{seq_lat}

\subsection{Remaining Tasks}
\subsubsection{Locks}
Exponential back-off will be added to each lock. According to Anderson, exponential back-off should reduce performance degradation under high contention \cite{anderson1990performance}. The performance of each lock with and without exponential back-off will be compared. The MCS queue lock will also be implemented, as it is regarded to be a scalable lock \cite{boyd2012non,mellor1991algorithms, valois1995lock}.
\subsubsection{Testing locks under contention}
New tests need to be implemented in order to gain new insights into the implemented locking algorithms. The performance of locks under contention will be measured using a similar methodology used in the spin-lock benchmark conducted by Intel \cite{intelxeonlockscaling}. A small critical section will be emulated by enqueuing and dequeuing a thread local circular buffer, reading and possibly invalidating some cache lines in the process, simulating a realistic work load. The latency observed will be a function of both re-entry time (the time each thread waits, after leaving the critical section \cite{intelxeonlockscaling}) and the critical section. I have opted to use re-entry time and the length of the critical section as the variables of the experiment to replicate the findings in Intel's benchmarks \cite{intelxeonlockscaling}, and measure locks which have not been covered by said benchmark.

\subsubsection{Queues}
A bounded and unbounded lock-based queue \cite[Section~10.2]{herlihy2020art} will be compared against various non-blocking queues. The aim of this comparison is to measure the cost of progress guarantees\cite{herlihy2020art, mckenney2017parallel} and the benefits they provide to scalability. The sequential latency of each queue's operations will be measured. The goal of this test is to observe if non-blocking queues can compete with blocking queues when under no contention.

Each queue will also be measured for throughput under contention \cite{valois1995lock,michael1996simple,kogan2011wait}. This test will by conducted on a fixed number of cores, with the re-entry time varying (the lower the re-entry time, the higher the contention). Observing how the average latency of each queue operation under different levels of contention is the goal of this test.

Finally, each queue's throughput under contention with random delay will be measured. Valois uses this measurement \cite{valois1995lock} when comparing blocking and non-blocking queues. The difference in performance between the blocking and non-blocking queues was stark, and did a good job of highlighting the difference between blocking and non-blocking queueing algorithms. Blocking algorithms tend to not only delay their own critical section, but also the other threads waiting for access to the lock. In order to simulate random delay, a random number of page faults will with a 10\% chance of occurring during any enqueue or dequeue.

% TODO: Mention that interferences haven't been removed yet
% TODO: Create to be done section
%   * Lock contention test
%   * Queue sequential latency
%   * Queue throughput under contention
%   * Queue throughput under contention with random delay.
% TODO: Mention deliverables
% TODO: Gantt chart.

\bibliography{references}
\bibliographystyle{ieeetr}
\end{onehalfspacing}
% ---------------   End  ----------------
\end{document}
