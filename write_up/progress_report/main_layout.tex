% Layout Author: Luca Muscat
% Website: www.github.com/lucamuscat
% Repository: www.github.com/lucamuscat/LaTex-Layout

\documentclass[a4paper, 12pt, titlepage]{article}

%--------------- Preambles ---------------
\usepackage[none]{hyphenat}
\usepackage{tikz}
\usepackage{aeguill}
\usepackage{setspace}
\usepackage{listings}
\usepackage[newfloat]{minted}
\usepackage[justification=centering]{caption}
\usepackage[margin=0.78in]{geometry}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{hyperref}

\setlength{\footnotesep}{\baselineskip}

\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newenvironment{code}{\captionsetup{type=listing}}{}
% \sourcecode{Directory/NameOfFile}{Caption}{Label}
% #1: Code Snippet
% #2: Caption
% #3: Reference Label
\newcommand{\sourcecode}[3]{
    \begin{code}
      \inputminted[linenos,numbersep=5pt,gobble=0,frame=lines,framesep=2mm,]{python}{#1}
        \caption{#2}
        \label{lst: #3}
    \end{code}
  }

\newcommand{\explainline}[2]{
  \textbf{Line #1} & #2 \\ \\
}

% \image{Scale}{Image Path}{Caption}{Reference Label}
% #1 - Scale
% #2 - Image path
% #3 - Caption
% #4 - Reference Label
\newcommand{\image}[4]{
  \begin{figure}[H]
    \centerline{\includegraphics[scale=#1]{#2}}
    \captionof{figure}{#3}
    \label{fig: #4}
  \end{figure}
}

\titlespacing{\section}{0pt}{2ex}{1ex}

%---------------  End   ---------------
\begin{document}

%---------------Header---------------
\begin{titlepage}
    \centering
    \includegraphics[width=4cm]{crest.pdf} % also works with logo.pdf
    \vskip1cm
    {\large University of Malta \\ Progress Report}

    \vfill
    \rule{\textwidth}{0.4pt}
    {\Large\textbf{A comparative study of lock-based \& lock-free concurrent queueing algorithms \& their performance}\\
    \rule{\textwidth}{0.4pt}
        \vskip2cm
        Luca Muscat (451401L) \\ Supervisor: Prof. Kevin Vella.
    }
    
    \vfill
    \vfill
    \vfill
\end{titlepage}

\title{A comparative study of lock-based \& lock-free concurrent queueing algorithms and their performance \\ \\ \large Progress Report}
\author{Luca Muscat (451401L) \\ luca.muscat.19@um.edu.mt \\ Supervisor: Prof. Kevin Vella}

\pagebreak
\pagenumbering{roman}
\tableofcontents
\pagebreak
\pagenumbering{arabic}

%---------------   End  ----------------

%--------------- Content ---------------

% ✔ Define wait-free and lock-free
% ✔ Define progress and safety
% ✔ Liveness vs progress, put backoff in context.
% ✔ What are the progress guarantees required for a queue
% ✔ Conditions that reflect that the queue is working correctly
% ✔ Progress - The state must be advance and progress
% ✔ Investigate progress and conditions w.r.t to queues
% ✔ Guide the structure of the lit review with sub sections (taxonomy, classify types of entity, in our case queue.)
% ✔ Find a good citation for a queue, baby citations, is there someone who classifies a queue? a concurrent one? Start with this
% ✔ Condense lines a bit more.
% ✔ Start numbering from introduction, not TOC.
% ✔ Mention that a subset of the algorithms will be implemented.

\begin{singlespace}
\section{Abstract}
  FIFO queues have been heavily researched throughout the last few decades. For a queue to be accessed and operated on concurrently, each operation needs to be synchronized with every thread in the system. Ill-synchronized operations may leave a queue in an inconsistent state. This dissertation aims to measure and characterize the performance of several distinct concurrent blocking and non-blocking queueing algorithms.

\section{Introduction and Motivation}
Knuth defines a queue as a linear list for which all insertions are made at one end of the list and all deletions are made at the other end \cite{knuth1968art}. A queue typically comes with two main operations, which are the enqueue (places an item at the head of the queue) and the dequeue (removes an item at the tail of the queue) operations.

Concurrent queues are often the basis of scheduling algorithms and many other concurrent algorithms. Blocking and non-blocking queuing algorithms tend to perform differently in different scenarios \cite{kogan2011wait,valois1995lock,hoffman2007baskets,preshinglockcontentionslow}; synchronization for non-blocking algorithms is expensive, as they require high latency atomic primitives. However, non-blocking algorithms also tend to be immune to performance degradation whenever random delay is present in the system \cite{valois1995lock}. 

A number of papers proposing the non-blocking queueing algorithms that will be studied in this project were measured in unrealistic or non optimal environments. Valois gathered readings on a parallel architecture simulator known as proteus \cite{valois1995lock}, other authors gathered readings in big runtime environments such as the Java Virtual Machine \cite{kogan2011wait, hoffman2007baskets}, leading to coarse measurements; neither Kogan et. al or Hoffman et. al provided a measurement for the average latency of their queue's operation, but provided the total run time of each benchmark (this total runtime also includes the overhead of the JVM). Fog states that runtime environments of this nature typically take more resources than the programs they are running \cite{fog2020optimizing}. Some of the papers mentioned \cite{hoffman2007baskets,kogan2011wait} also tend to compare the performance of their algorithms with a limited number of non-blocking algorithms, meaning that they are unable to determine if their implementation is competitive with blocking queueing algorithms.

There are studies which already exist that measure and compare the performance of non-\\blocking algorithms with blocking ones \cite{valois1995lock, michael1996simple}, however, they are either outdated, or do not include some of the more contemporary non-blocking queuing algorithms.

\section{Why is this problem non-trivial?}
Multiprocessor programming is considered to be an art \cite{herlihy2020art}, as it is difficult to do correctly. The performance of concurrent queueing algorithms is heavily dependent on the CPU and main memory. This means that when doing parallel programming, it is difficult to ignore the hardware, as performance and scalability may easily degrade due to the non-optimal usage of the hardware's resources, such as causing cache misses, saturating cache lines, page faults and false sharing, among other things \cite{herlihy2020art, mckenney2017parallel}. It is easy to implement an incorrect non-blocking algorithm, as the correctness of such algorithms relies on subtleties such as memory ordering, memory barriers and synchronization primitives that are difficult to understand intuitively (especially without prior experience to parallel programming).

\section{Background research and literature review}
\subsection{Mutual Exclusion}
A blocking concurrent queue provides concurrency through mutual exclusion; guaranteeing that a process's critical actions does not overlap with critical actions from different processes \cite{herlihy2020art,valois1995lock}. 

Any mutual exclusion protocol needs to determine what to do when it is unable to acquire a lock \cite[Chapter~7]{herlihy2020art}. There are three alternatives to this problem \cite[Chapter~7]{herlihy2020art}:
\begin{itemize}
  \item Busy waiting - Conditionally and actively polls an area in memory or a shared resource. Busy waiting locks are typically referred to as spin-locks.
  \item Blocking - Yields control of the CPU to other processes.
  \item Hybrid - Makes use of both busy waiting and blocking for optimal use of resources.
\end{itemize}

This project will only consider busy waiting algorithms.

\subsubsection{Test and Set}
Test and set is the simplest spin-waiting algorithm where each processor tries to repeatedly execute a test and set instruction until it succeeds to acquire the lock \cite{anderson1990performance}. The test and set algorithm is a classic example of an algorithm that experiences performance degradation due to contention \cite{anderson1990performance, graunke1990synchronization,boyd2012non}.

\subsubsection{Test and test and set}
Segall and Rudolph \cite{rudolph1984dynamic} proposed an alternative to the test and set (TAS) spinning method called test-and-test-and-set (TTAS). 
TTAS reduces the amount of cache line invalidations caused by TAS by checking if the flag being polled has changed before calling TAS.

Anderson proposed an improvement to the TTAS lock by implementing several spin locks based on CSMA network protocols \cite{anderson1990performance}. Anderson noted that spin locks that used Ethernet's back off protocol tended to perform better under contention than a regular TTAS lock.

Graunke and Thakkar also proposed improvements to the TTAS lock by adding a delay after each failed test in order to reduce contention \cite{graunke1990synchronization}. The authors also proposed a novel queuing lock that outperformed all variations of the TAS lock when more than three processors competing for access.

\subsubsection{MCS Lock}
Mellor-Crummey and Scott propose a novel array based lock known as the MCS lock that outperformed both array based queueing locks proposed by Anderson and Graunke et al. \cite{mellor1991algorithms}. Insight into existing spin locks such as TAS, TTAS, and the ticket lock were also given.

\subsection{Non-blocking algorithms}
\subsubsection{Progress Conditions}
There are two types of progress conditions, which are blocking and non blocking \cite[Chapter~3.7]{herlihy2020art}. An algorithm is blocking if an unexpected delay in one thread can prevent other threads from making progress, whilst a non-blocking one will at no point in time wait for another thread \cite[Chapter~3.7]{herlihy2020art}. Two types of blocking progress conditions are deadlock-free and starvation-free. These properties offer guarantees that every thread will eventually leave every critical section in a timely manner \cite[Chapter~3.7.1]{herlihy2020art}. 

There are different types of non-blocking progress conditions, which may either be dependent or independent of other threads. 

A method is obstruction-free if, from any point after which it executes in isolation, it finishes in a finite number of steps (may be implemented using back-off algorithms whenever conflict is encountered)\cite{herlihy2020art,herlihy2003obstruction}. Obstruction-free algorithms are, by definition, dependent.

A method is lock-free if it guarantees that infinitely often some method call finishes in a finite number of steps \cite{herlihy2020art}.

A method is wait-free if it guarantees that a method call finishes in a bounded number of steps. This bound may also depend on the number of threads \cite{herlihy2020art}.

\subsubsection{Correctness Conditions}
Correctness conditions describe the pre and post conditions for a concurrent object's operations \cite{herlihy2020art}. Such pre and post conditions decide whether a concurrent history is legal \cite{herlihy1990linearizability}. Correctness conditions are usually based on two requirements: (1) When an operation takes effect, and (2) how the order of non concurrent operations should be preserved \cite{herlihy1990linearizability}.

\subsubsection{Correctness conditions with respect to queues}
The defacto correctness condition for a queue is linearizability, as it ensures that the semantics of the queue's operations are not altered \cite{mellor1987concurrent}. A queue cannot be sequentially consistent, as sequential consistency allows histories that don't lead to FIFO ordering \cite{ herlihy2020art, herlihy1990linearizability}. A queue may also be serializable, however, serializability constrains the degree of concurrency that can occur, making it too strong of a correctness condition \cite{herlihy2020art,herlihy1990linearizability}.

When talking about correctness conditions with respect to FIFO queues, restricting the order of method calls is only a subset of the applicable correctness conditions. Yahav and Sagiv\cite{yahav2003automatically} provide some examples of correctness conditions for concurrent FIFO queues, which are: (1) The linked list is always connected, (2) Nodes are only inserted after the last node of the linked list, (3) Nodes are only deleted from the beginning of the linked list, (4) The head of the queue always points to the first node in the linked list, (5) The tail of the queue always points to a node in the linked list.

\subsubsection{Valois's Queue}
Valois covered multiple lock-free queue implementations that utilized linked lists, noting that most of the algorithms could be implemented using the Compare and Swap (CAS) atomic primitive. Valois noted that CAS based algorithms suffer from the ABA problem \cite{valois1994implementing}. 

Valois proposed the safe read protocol, which was a solution to the ABA problem that did not require stronger versions of CAS (such as double compare and swap) primitive \cite{valois1994implementing, valois1995lock}. A new lock free queue that made use of arrays and CAS was also proposed. The novel lock-free queue outperformed most lock-based queues both sequentially and under contention. Valois measured the performance of each lock-free queue with and without protection against the ABA problem, noting that even with the overhead, lock-free queues were still competitive with their lock-based counterparts.

Most notably, Valois measured the performance of each queue affected by random delay. Valois did this by simulating a delay of a random amount of cycles following an exponential distribution with a mean of 1000 cycles with a 10\% chance of occurring. The results showed that the response time of the lock-free implementations were approximately 17\% of that of the spin lock protected queues, highlighting the superiority of lock-based algorithms in environments susceptible to random delay. The massive difference in performance was attributed to the blocking nature of the spin lock, noting that a delayed spin lock not only affected the current operation, but also the other pending critical sections.

Valois also concluded that spin lock algorithms require some type of back off mechanism in order to decrease the degraded performance caused due to cache line saturation.

\subsubsection{The Baskets Queue}
Hoffman et. al propose a novel linearizable lock-free queue that took advantage of the fact that ordering amongst overlapping linearizable operations is non-deterministic \cite{hoffman2007baskets}. 

Each overlapping operation is grouped together in a ``basket'' (list of nodes). Nodes in a basket could be dequeued without order. A thread places an object inside a basket when a compare and swap fails due to another thread being in progress.

\subsubsection{Lock-free queue using elimination}
Shavit et. al introduced a novel lock-free queue using a technique known as elimination \cite{moir2005using}. 

Simply put, elimination works by allowing opposing operations such as enqueues and dequeues to exchange values without synchronizing on a shared data structure \cite{shavit1997elimination, moir2005using}. The results obtained suggests that as levels of concurrency increases, so does the throughput of the proposed data structure.

\subsubsection{Kogan and Petrank's wait-free queue}
\label{kp-queue}

Kogan et. al proposed a novel and efficient wait-free queue that was based on link-lists and thread helping (ie. when threads help other threads by completing their intended operation whenever conflict or delay is encountered)\cite{kogan2011wait}.
The proposed wait-free queue assumed that a garbage collector was responsible for preventing the ABA problem. Since a wait-free garbage collector did not exist, Kogan et al proposed that the hazard pointer technique could be used in solving the ABA problem.
\subsubsection{Benchmarking}
Fog and McKenney offered insights into the reduction of errors and interferences when bench-\\marking \cite{fog1996optimizing,fog2020optimizing, mckenney2017parallel}. Measures, such as keeping the CPU clock frequency stable, ignoring the first few iterations due to the cache and the branch predictor being cold\cite{fog1996optimizing} (ie. cache misses and branch mispredictions always occur in the first few iterations of the benchmark), avoiding symmetric multi-threading (aka. hyper-threading) \cite{fog2020optimizing} and detecting kernel interferences \cite[Chapter~11.7]{mckenney2017parallel}, were all suggested to improve the validity of a benchmark.

\section{Aims and objectives}
The aim of this project is to compare and characterize the performance of blocking \cite{anderson1990performance, herlihy2020art,graunke1990synchronization} and non-blocking \cite{herlihy2020art,valois1995lock} concurrent queueing algorithms. This will be achieved by gathering different measurements related to the performance of each queueing algorithm under study in different conditions. The goal of measuring performance under different conditions is to highlight the costs and benefits of each queueing algorithm's progress guarantees and correctness conditions. A chapter of this project will be dedicated to analyzing different locking algorithms and their performance, the aim of which is also to measure the costs of certain progress and correctness conditions. If access to hardware that supports certain performance event counters is feasible, this paper will also look into measurements such as data sharing, cache misses and cache line invalidations \cite{intelmanual,intelmanualoptimization,sahelices2009methodology} to provide more empirical evidence to specific claims.

\section{Methods and techniques used/planned}
The methodology planned to carry out this project starts by surveying currently existing blocking and non-blocking queuing algorithms. Next, several parallel programming techniques and pitfalls will be investigated. After the investigation is finalized, a number of locks mentioned in the literature review will be implemented. Finally, a subset of the non-blocking queues mentioned in the literature review will be implemented and analyzed, together with blocking queue algorithms using a variety of the locks implemented.

\section{Evaluation Plan}
Every lock will be measured for sequential latency (performance of the lock without contention). It is expected that the TAS and the TTAS locks will outperform the filter lock and the MCS lock as the filer lock offers starvation freedom \cite{herlihy2020art} and the MCS lock aims for deadlock-freedom and fairness amongst other threads \cite{mellor1991algorithms}.

The throughput under contention of each lock will also be measured. Throughput under contention shows the scalability of each lock and shows the degradation in performance due to contention.

Each queue's operations will be measured for sequential latency. Sequential latency is an important metric as it gives an overview of each operation's implementation complexity and running time \cite{valois1995lock}. 

Throughput under contention will be measured. The goal of this measurement is to show what degree of contention is required for non-blocking queue to be competitive with a blocking one. 

Finally, throughput under contention with random delay will be measured. The goal of this measurement is to show the superiority of non-blocking algorithms in systems where contention is difficult to control. It is expected that the non-blocking queues will outperform blocking queues, since delay inside of a blocking algorithm will also cause delay for every waiting thread. 
\section{Deliverables}
\begin{itemize}
  \item The Final Year Report, which will include the required background information for the reader to understand the project, together with a detailed explanation of the results obtained.
  \item The implementations of the locks, queues, and benchmarking systems developed. 
  \item Short documentation on using and configuring the benchmarks in order to reproduce the results.
\end{itemize}

\section{Timeline}
The work done in this FYP will be split into phases/milestones. Starting on 04-02-2022, the first phase (phase A) will span two weeks and will consist of implementing the test and set lock, test and test and set lock, filter lock, and the MCS lock. Sequential latency and latency under contention will also be measured. The test machine will also be setup and fine tuned to reduce interference with the benchmarks. 

Phase B will span 5 weeks, a week of which will be dedicated to implementing the blocking queues, followed by another two week of implementing a subset of the non-blocking queues mentioned in the literature review. Another 2 weeks will be dedicated to implementing the queue benchmarks and fixing any bugs that may arise.

Lastly, phase C will include the write up, which starts on 25-03-2022 and ends once the FYP project is submitted. 

Figure \ref{fig: gantt} visually explains the timeline using a Gantt chart. Also note that so far, several locks have already been implemented and their sequential latencies have been gathered.
\pagebreak
\appendix
\bibliography{references}
\bibliographystyle{ieeetr}
\pagebreak
\section{Gantt Chart}
\image{0.8}{gantt_chart.png}{Gantt chart that includes the time allocated for each phase, together with the relevant deadlines, symbolized by a rotated black square.}{gantt}
\end{singlespace}
% ---------------   End  ----------------
\end{document}
