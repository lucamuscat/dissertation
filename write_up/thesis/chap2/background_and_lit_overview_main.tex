\chapter{Background}

% [] Define what a multiprocessor environment is, and how it differs from a single processor environment.
% [] Define concurrent objects (using the text that already exists).
% [] Correctness conditions
% [] Progress conditions
% [] CPU Architectures, and why they should be taken into consideration.
% [] Mutual Exclusion


% \section{Threads}
% % [] What is a thread and why is it used?
% % [] Formal definitions:
% % Sequential Programs, Concurrent Programs, Threads, Concurrent Systems

% \subsection{Concurrent Objects}
% Herlihy defines a concurrent object as a data object shared by concurrent processes\citep{herlihy1990linearizability}. 

% Concurrent systems are modelled by a finite sequence of method invocations and response events\citep[Chapter~3.6]{herlihy2020art}. Herlihy provides the following formal definitions\citep[Chapter~3.6]{herlihy2020art}:
% % \begin{itemize}
% %   \item A method call in a history \emph{H} is a pair made up of an invocation that follows the next response (An invocation is the starting point of the method call, whilst the response may be seen as a point in the method where an object is returned).
% %   \item An invocation is pending in \emph{H} if it does not follow a matching response.
% %   \item A history \emph{H} is \emph{sequential} if the first event of \emph{H} is an invocation, and each invocation, possibly excluding the last, is immediately followed by a matching response.
% %   \item A history \emph{H} is \emph{well formed} if all of its sub-histories are \emph{sequential}.
% %   \item A thread sub-history, $H|A$  (``\emph{H at A}'') is the subsequence of all events in \emph{H} whose thread names are \emph{A}.
% %   \item Two histories \emph{H} and \emph{H'} are equivalent if $\forall a:A \cdot H|A = H'|A$.
% % \end{itemize}
% \begin{definition}[Method Call]
% A method call in a history \emph{H} is a pair made up of an invocation that
% follows the next response; An invocation is the starting point of the
% method call, whilst the response is the point in the method where an object is
% returned.
% \end{definition}

% \begin{definition}[Pending Invocation]
% An invocation is pending in \emph{H} if it does not follow a matching response.    
% \end{definition}

% \begin{definition}[History]
% A history \emph{H} is \emph{sequential} if the first event of \emph{H} is an
% invocation, and each invocation, possibly excluding the last, is immediately
% followed by a matching response. A history \emph{H} is \emph{well formed} if
% all of its sub-histories are \emph{sequential}.
% \end{definition}

% \begin{definition}[Sub-History]
% A thread sub-history, $H|A$  (``\emph{H at A}'') is the subsequence of all
% events in \emph{H} whose thread names are \emph{A}.
% \end{definition}

% \begin{definition}[Equivalence]
% Two histories \emph{H} and \emph{H'} are equivalent if $\forall a:A \cdot H|A =
% H'|A$.
% \end{definition}

\section{Multiprocessors}
The diminishing growth in \emph{uniprocessor} speeds
forced programmers to look at parallelism as a means of improving
performance~\citep{cantrill2008real}. 

Unlike uniprocessors, \emph{multiprocessors} do
not ensure that instructions appear in execution order~\cite{scott2013shared},
making multiprocessors hard to model.

% \emph{Multiprocessors} consist of multiple hardware processors, each
% executing a \emph{sequential program}~\citep[Appendix~B.2]{herlihy2020art}.

\emph{Multiprocessors} execute a number of \emph{processes} on distinct
hardware processors~\citep[Appendix~B.2]{herlihy2020art}, with each processor
continuously executing, and swapping out \emph{processes}.
\citeauthor{scott2013shared} defines a \emph{process} as a set of
\emph{threads}, where a thread is an active computation that has the potential
to share variables with other, concurrent threads~\citep[p.6]{scott2013shared}

Once a process' \emph{time quantum} (fixed share of CPU time) has expired, the operating system
uses a \emph{context switch} to swap the process with another. A process
may be \emph{de-scheduled} before its time quantum has expired due to interrupts,
system calls, or through the voluntary yielding of control to the CPU~\citep[Section~3.2.3]{osconcepts2021}.

Over the years, multiprocessor programming has become more practical and
feasible, partly thanks to multiprocessors getting cheaper, and
easier to program due to abstractions such as OpenMP, MPI (Message Passing
Interface), implicit parallel programming environments such as SQL, and
programming language standards that provide guarantees to how parallel code will
behave, such as Java's\citep{javamemorymodel2014} and C++11's
\citep{cppmemorymodel} standardized memory models
\citep[Chapter~2.2]{perfbook2021}.

% \citep[Appendix~B.7.1]{perfbook2021}.
For the sake of optimization, CPUs and compilers do not guarantee any specific
results when multiple threads modify the same memory location
concurrently~\citep{drepper2007every}; 
an example of an optimization is the out-of-order
execution of instructions~\citep[Appendix~B.7.1]{perfbook2021}.

Developers of multiprocessor
programs need to explicitly make use of synchronization
primitives in order to prevent data from becoming corrupted or inconsistent
amongst processors.

\section{Synchronization}
\subsection{CPU Cache}
As the disparity between CPU frequency and memory access times grew, 
CPU engineers added small, yet fast SRAM chips between the CPU
and the main memory to alleviate issues caused by the speed
differences~\citep{cantrill2008real,drepper2007every,perfbook2021}. Using large
amounts of SRAM is considered to be uneconomical, therefore, more cache layers
were introduced, with each layer being cheaper, larger, and slower than the
last~\citep{drepper2007every,perfbook2021}.

Data is transferred in units of \emph{``cache lines''}, which are power-of-two
fixed-size aligned blocks of memory, commonly ranging from 32 to 256 bytes in
size~\citep[Section~3.2.1]{perfbook2021}. Cache lines are filled with data from the requested memory address,
together with neighbouring bytes~\citep[Section~3.2.1]{perfbook2021}.

% The \emph{MESI} cache-coherence protocol (Modified, Exclusive, Shared, Invalid)
% attaches states to memory addresses; transitions notify the processor
% when to invalidate a cache line in order to prevent stale data~\citep[Appendix~B.5.1]{herlihy2020art}.

\emph{Cache-Coherence} is a problem where different caches contain stale
(out-of-date) values~\citep[Appendix~B.5.1]{herlihy2020art}. A memory address
may only be safely cached in different processors if no updates
occur on the shared data; cache lines will be
invalidated if said data is updated (preventing stale data from being read)~\citep[Appendix~B.5.1]{herlihy2020art}. 

The \emph{MESI} cache-coherence protocol (Modified, Exclusive, Shared,
or Invalid) attaches states to memory addresses~\citep[Appendix~B.5.1]{herlihy2020art}. Transitions in
state notify the processor if a cache line is to be invalidated in order to
prevent stale data~\citep[Appendix~B.5.1]{herlihy2020art}.

%\emph{Sharing} occurs when overlapping data is loaded into several cache lines.
\emph{Sharing} occurs when one processor reads or writes to a memory address that
is cached by another~\citep[Appendix~B.5.1]{herlihy2020art}. In some cases, sharing is unavoidable; it is possible for closely located, yet
logically distinct data to share a single cache
line~\citep[Appendix~B.5.1]{herlihy2020art}. 

\emph{False Sharing} occurs when a cache line is invalidated due to an update
on logically distinct data~\citep[Appendix~B.5.1]{herlihy2020art}. 

In parallel programming, \emph{false sharing} is a
performance killer, as writes become expensive due to the increased rate of
cache line invalidations. At the cost of a higher memory footprint, \emph{false
sharing} may be thwarted by padding data with unused memory to prevent cache
lines from being shared~\citep{scott2013shared}.

\subsection{Interconnection}

% TODO: Not really happy with the UMA definition, will need to give a better explanation.
An interconnection medium is used to connect CPUs to cache and memory; where
two kinds of interconnection architectures exist: (1)
\emph{Nonuniform memory access (NUMA)}
architectures~\citep[Appendix~B.3]{herlihy2020art} are typically associated with
multi-CPU and distributed systems; (2) \emph{Uniform memory
access (UMA)} architectures link processors and memory using a bus
interconnect~\citep[Appendix~B.3]{herlihy2020art}.

The cognizance of interconnection architecture is important,
as some algorithms are not suited for different architectures due to
overheads associated with accessing memory. This study will be
conducted on a \emph{UMA} architecture.

\subsection{Atomic Operations}
% TODO: Mention how CPUs can perform atomics semantically, as opposed to
% performing it in hardware.
% TODO: Find reference for the word size line.
An operation is said to be \emph{atomic} if it is impossible to observe an
intermediate state~\citep{perfbook2021}. On most architectures, atomic
operations may only affect the CPU's word size worth of memory.

\emph{Read-Modify-Write (RMW)} operations read and write to memory
atomically~\citep{perfbook2021}. Most synchronization primitives may be
implemented as RMW operations~\citep[Section~5.6]{herlihy2020art}, and
typically need to be implemented in hardware~\citep[Appendix~B.8]{herlihy2020art}.
\citeauthor{herlihy1991wait} proved that not all synchronization primitives are
equally powerful by associating a \emph{consensus number} (i.e.~the number of
processes that the \emph{consensus problem} can be solved for) to each
synchronization primitive. \emph{Non-atomic} operations are prone to \emph{load} and \emph{store
tearing}~\citep[Section~4.3.4]{perfbook2021}\footnote{The Intel and ARM
architectures guarantee that correctly aligned loads and stores are
atomic~[\citealp{intel2021system},~Section~8.1.1;
\citealp{arm2022architecture},~Chapter~2.2].}.

\citeauthor{scott2013shared} lists a number of RMW operations~\citep[Table~2.2]{scott2013shared}.
\emph{Compare-and-swap} (\emph{CAS}, also known as \emph{Compare and Exchange})
is a ternary atomic operation that requires: A source, destination, and an
update value. If the source and destination are equal, the source is set to the
update value, otherwise, the destination is updated with the source's
value~\citep{intel2021inst}.

\begin{algorithm}
    \caption{x86 compare-and-swap pseudocode.}\label{alg:cas}
    \begin{algorithmic}[1]
        \Function{compare-and-swap}{ptr<T> Source, ptr<T> Destination, T UpdateValue}
            \If{$Source~=~Destination$}
                \State $Source~\gets~UpdateValue$
                \State return True
            \EndIf
            \State $Destination~\gets~Source$ \label{alg:line:sourceneqdest}
            \State return False
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Older studies may give different definitions of
CAS~\citetext{\citealp{scott2013shared},~Table~2.2;~\citealp{valois1995lock},
Appendix~A}, such that line \ref{alg:line:sourceneqdest} is excluded.

\subsection{ABA Problem}
\citeauthor{dechev2010understanding} define the ABA problem as a false positive
execution of a CAS on a shared location~\citep{dechev2010understanding}. The ABA
problem may occur when the source's memory address has been changed from $A$ to
$B$, and back to $A$, bearing in mind that $A$ has been freed and immediately
allocated a logically distinct value, between the point in time when the
destination was loaded and the CAS was executed~\citep{dechev2010understanding}.

% [x] Caching is needed to avoid accessing slow memory by copying data into faster memory hardware
% [x] There are different levels cache, there is the register, L1, L2, and L3
% As cache levels move towards the register, access times get faster and cache sizes get smaller since they are more expensive.
% [x] CPU cache lines are used as a unit of transfer between different caches, on most architectures, a cache line is 64 bytes in size.
% [x] Mention what false sharing is and why it should be avoided.
% [] Mention how false sharing impacted this study's performance before being fixed.

\subsection{Progress Conditions}
A \emph{progress condition} is a \emph{liveness property} that describes how,
and when a number of threads are able to make \emph{forward
progress}~\citep[Section~3.2]{scott2013shared}.

A concurrent method is said to be \emph{blocking} if there is some reachable
state in the system where a thread cannot make progress until some other thread
takes action~\citep[Section~3.2]{scott2013shared}. Algorithms that rely upon
mutual exclusion are considered to be
blocking~\citep[Section~3.2]{scott2013shared}, however, non-lock-based
algorithms may also be blocking~\citep{mellor1987concurrent}.

A concurrent method is said to be \emph{non-blocking} if there is no reachable
state in the system where a thread cannot make
progress~\cite[Section~3.2]{scott2013shared}. The non-blocking property is
desired due to its immunity to random delays. % TODO: Find citation

The following are a subset of non-blocking progress conditions, ordered by the
strength of the property (starting from the weakest) and the complexity of
implementation (starting from the simplest):

\begin{enumerate}
\item \emph{Obstruction-Freedom}: A method is obstruction-free if, from any
point after which it executes in isolation, it finishes in a finite number of
steps~\citep[Section~3.8.3]{herlihy2020art};
\item \emph{Lock-Freedom}: A method is lock-free if some method call finishes
in a finite number of steps~\citep[Section~3.8.2]{herlihy2020art};
\item \emph{Wait-Freedom}: A method is wait-free if every call finishes its
execution in a finite number of steps~\citep[Section~3.8.1]{herlihy2020art}.
\end{enumerate}

Stronger guarantees do not necessarily equate to higher throughput, as the
overheads and complexity of the implementation tends to be positively
correlated to the strength of the guarantee.

\subsection{Correctness Conditions}
\label{sec:correctness_conditions}
\emph{Correctness conditions} describe the pre and post conditions for a concurrent
object's operations~\citep{herlihy2020art}. Such conditions decide
whether a concurrent history is legal \citep{herlihy1990linearizability}.
Correctness conditions are based on two requirements: (1) When an
operation takes effect, and (2) how the order of non-concurrent operations
should be preserved \citep{herlihy1990linearizability}.

A method is said to \emph{take effect} when the effects of its method call is
seen by other method calls~\citep[Section~3.4.1]{herlihy2020art}. 

% Find a formal definition of "takes effect"
\emph{Concurrent systems} may be modelled as a \emph{history}, which is a
sequence of events, where each event is either an \emph{invocation} or a
\emph{response}~\citep[Section~3.6.1]{herlihy2020art}. 

A history is said to be \emph{legal} with respect to a correctness property if
it is \emph{equivalent} to a history that respects said correctness property.

\citeauthor{lamport1979make} defines the \emph{sequential consistency}
correctness condition as a history in which the result of an execution is the
same as if the operations had been executed in \emph{program
order}~\citep{lamport1979make}.

A \emph{linearizable} concurrent computation gives the illusion that a method
call takes effect instantaneously some time between the method's invocation and
response; the point in time when the method takes effect is also known as the
\emph{linearizability point} \citep{herlihy2020art,herlihy1990linearizability}.
Processors perceive linearizable operations in a \emph{total, and real-time
order}, where overlapping method calls take effect in a non-deterministic
order~\citep[Section~3.6.2]{herlihy2020art}.


% A subset of existing correctness conditions are:

% \begin{itemize}
% \item \emph{Sequential}: Sequential consistency requires that each process's method call takes effect in program order.
% \item \emph{Linearizable}: A concurrent computation is linearizable if it is ``equivalent'' (as previously formally defined) to a legal sequential computation \citep{herlihy1990linearizability}. A linearizable concurrent computation gives the illusion that a method call takes effect instantaneously some time between the method's invocation and response; the point in time where the method takes effect is also known as the linearizability point \citep{herlihy2020art,herlihy1990linearizability}. Linearizability is composable, meaning that if each object in a system satisfies a condition, the entire system also satisfies said condition \citep[Chapter~3.3.1]{herlihy2020art}. Linearizability is also a non-blocking property, which means that method call is never forced to wait \citep{herlihy1990linearizability}. A problem with linearizability is that the order of overlapping operations is non-deterministic.
% \item \emph{Serializable}: A history is serializable if it is equivalent to a history where transactions appear to happen sequentially (without interleaving) \citep[Section~3.3]{herlihy1990linearizability}. This correctness condition is typically used in databases and distributed systems \citep{guerraoui2019consensus}. Serializable consistency comes with the blocking progress condition and is a general case of linearizability \citep{herlihy1990linearizability}.
% \end{itemize}

\subsection{Memory Consistency Models}
\emph{Memory Consistency Models (referred to as memory models henceforth)} are correctness properties that describe how
threads perceive the order of other threads' effects on shared
memory~\citep[Section~3.7]{herlihy2020art}. Similarly to the correctness
conditions described in section~\ref{sec:correctness_conditions}, memory
models vary in strength; as the model used becomes more relaxed, more aggressive
optimizations may be used, increasing performance at the cost of
programmability and portability~\citep{gharachorloo96consistency}.

\emph{Relaxed memory models} such as \emph{weakly ordered models}~\footnote{A
number of ARM processors tend to be weakly ordered when interacting with main memory~\citep[Section~A3.5.5]{arm2022architecture}} offer little to no
constraints with respect to the ordering of reads and writes; the explicit use
of memory barriers are needed for finer grained control over the ordering of
reads and writes~\citep{gharachorloo96consistency}. \citeauthor{gharachorloo96consistency} identifies relaxed
memory models as \emph{system-centric models}, as they tend to favour
performance, as opposed to a programmer's intuition about shared-memory
(sequential consistency)~\citep{gharachorloo96consistency}.

Memory models such as sequentially consistent models are considered to be
\emph{strongly ordered}. Strongly ordered systems tend to
heavily restrict the reordering of instructions, giving a simpler and
higher-level interface between memory and the programmer. 


% [] What is a memory consistency model?
% [] Sequential Consistency
% [] Weak Consistency
% [] ARM is weakly consistent when writing to normal memory.
% [] Xbox360 CPUs are sequentially consistent as they reads and writes are in order (be careful, as in order might not be sequential consistence).
% [] Modern Intel CPUs use a variation of sequential consistency known as program-ordered-consistency
% [] Give the multithreaded counting example, and mention how torn reads/writes and incoherent caches may lead to incorrect results.

% \section{Queues}
% \subsection{Definition}
% Knuth defines a queue as a linear list for which all insertions are made at one end of the list and all deletions are made at the other end \citep{knuth1968art}. A queue typically comes with two main operations, which are enqueue (places an item at the head of the queue) and dequeue (removes and returns an item at the tail of the queue). Queues follow First in First out (FIFO) ordering (unlike the stack, which is Last in First out).
% \subsection{Definition of a concurrent queue}
% A concurrent queue is a type of queue that remains consistent and correct when accessed simultaneously through different threads. Non-concurrent or ill-synchronized queues typically end up in an inconsistent state after being accessed through multiple threads (this may occur for a number of reasons, such as race conditions)\citep{yahav2003automatically}, this may lead to undefined and incorrect behaviour. Concurrent queues are often the basis of scheduling algorithms and many other concurrent algorithms \citep{yahav2003automatically}.
% % \subsection{Correctness conditions with respect to queues}
% % The defacto correctness condition for a queue is linearizability, as it ensures that the semantics of the queue's operations are not altered \citep{mellor1987concurrent}. A queue cannot be sequentially consistent, as sequential consistency allows histories that don't lead to FIFO ordering. A queue may also be serializable, however, serializability constrains the degree of concurrency that can occur, making it too strong of a correctness condition.

% % When talking about correctness conditions with respect to FIFO queues, restricting the order of method calls is only a subset of the applicable correctness conditions. Yahav and Sagiv\citep{yahav2003automatically} provide some examples of correctness conditions for concurrent FIFO queues, which are:
% % \begin{itemize}
% %   \item The linked list is always connected
% %   \item Nodes are only inserted after the last node of the linked list
% %   \item Nodes are only deleted from the beginning of the linked list
% %   \item The head of the queue always points to the first node in the linked list
% %   \item The tail of the queue always points to a node in the linked list.
% % \end{itemize}

% \section{Mutual Exclusion}
% The 'Mutual Exclusion' protocol ensures that shared memory remains consistent when modified concurrently by two or threads running in parallel.

% % Mutual exclusion introduces the concept of critical sections, where different threads do not overlap \citep[Chapter~2]{herlihy2020art}. 

% Any mutual exclusion protocol needs to determine what to do when it is unable to acquire a lock. There are three alternatives to this problem \citep[Chapter~7]{herlihy2020art}:
% \begin{itemize}
%   \item Busy waiting - Conditionally and actively polls an area in memory or a shared resource. Busy waiting locks are typically referred to as spin-locks.
%   \item Blocking - Yields control of the CPU to other processes.
%   \item Hybrid - Makes use of both busy waiting and blocking for optimal use of resources.
% \end{itemize}

% ``Roll your own lightweight mutex''\citep{preshingmutex} covers a simple mutex implementation using a type of semaphore informally known as the `Benaphore'\citep{haikubenaphore}.

% ``Locks Aren't Slow; Lock Contention Is''\citep{preshinglockcontentionslow} offers insight into the performance of spin locks under contention. Preshing uses a custom implementation of the Mersenne Twister \citep{matsumoto1998mersenne} to simulate a critical section. The workload (ie. the amount of time that the lock is held for) and the number of threads used as variables whilst the lock frequency remained constant. The results show that high levels of contention on a lock is enough to degrade the performance of a parallel solution to the point where a sequential thread will perform better.

% Boyd-Wickizer et. al show that non-scalable locks, such as spin locks should not be used in operating systems where contention is hard to control. Several micro-benchmarks were implemented using a spin lock implementation offered by the linux kernel; as the contention increased, the performance of each benchmark dropped drastically. The spin lock was then re-implemented using scalable locks, such as the MCS lock \citep{mellor1991algorithms} and the CLH lock \citep{craig1993building,magnusson1994queue} improved performance on a large number of cores by at least 3.5 times, and in some cases, 16 times.

% Segall and Rudolph \citep{rudolph1984dynamic} propose an alternative to the test and set (TAS) spinning method called test-and-test-and-set (TTAS). TTAS reduces the amount of cache line invalidations caused by TAS spinning by checking if the flag being spinned on has changed before calling TAS.

% Anderson offers an improvement to the TTAS lock by implementing several spin locks based on CSMA network protocols \citep{anderson1990performance}. Anderson notes that spin locks using Ethernet's back off protocol tends to perform better under contention than a regular TTAS lock.

% Graunke and Thakkar also offer improvements to the TTAS lock by adding a delay after each failed test in order to reduce contention \citep{graunke1990synchronization}. The authors also proposed a novel queuing lock that outperformed all variations of the TAS lock when more than three processors are competing for access.

% Mellor-Crummey and Scott propose a novel array based lock known as the MCS lock that outperforms both array based queueing locks proposed by Anderson and Graunke et al. \citep{mellor1991algorithms}. The authors also offer insight into existing spin locks such as TAS, TTAS, and the ticket lock, and describe their benefits and their caveats.

% \section{Benchmarks and Performance Analysis Methodologies}
% Fog and McKenney offer insights into how to reduce the number of errors and interventions when micro-benchmarking \citep{fog1996optimizing,fog2020optimizing, perfbook2021}. Some measures to prevent errors are: keeping the CPU clock frequency stable (Stinner described methods of configuring and reading multiple CPU parameters \citep{stinnerpstate}), ignoring the first few iterations due to the cache and the branch predictor being cold\citep{fog1996optimizing}, avoiding symmetric multi-threading (aka. hyper-threading) \citep{fog2020optimizing} and detecting kernel interferences \citep[Chapter~11.7]{perfbook2021}.

% Intel's lock scaling analysis on Intel Xeon processors \citep{intelxeonlockscaling} benchmarks the relative contended performance of the Xeon Phi E5-2600 over X5600. The most notable contribution of this paper is the methodology used to for the benchmarks. The authors claim that a microbenchmark that aims to study lock performance does not reflect behaviour on real software if the study does not factor in the length of the simulated critical section and the frequency of locking. The paper reaches the conclusion that when predictable performance is desired, the locking algorithm requires reasonably long critical sections and re-entry times.

% Sahelices et al. offer a new methodology for tuning performance and critical sections inside of parallel programs \citep{sahelices2009methodology}. Critical sections were characterized by lock contention and degree of data sharing, allowing the authors to identify a number of inefficiencies caused by data sharing patterns and data layouts. Interestingly, the benchmarks were conducted on a multiprocessor simulator called RSIM, that allowed the authors to take fine grained and accurate statistics.

% Gregg describes several performance 'Anti-Methodologies' and 'Methodologies' \citep{methodologygregg}. Anti-Methodologies are methods of benchmarking performance that do not lead to accurate or correct results. This is an article that anyone in the field of performance analysis should read, as it reveals methodical and structured methods of carrying out performance analysis.

% Intel's optimization reference manual \citep{intelmanualoptimization} suggests several potential metrics that may be derived from specific hardware counters. Notably, equations for calculating bus utilization, L2 Modified Lines Eviction Rate, and Modified Data Sharing Ratio are all provided.
